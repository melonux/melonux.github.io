[
  {
    "objectID": "posts/univariate_distribution_relationships/index.html",
    "href": "posts/univariate_distribution_relationships/index.html",
    "title": "[译注] 单变量概率分布之间的关系",
    "section": "",
    "text": "在传统的数理统计教科书中，概率分布往往被孤立的介绍，很少讨论他们之间的联系。本文对 Leemis 在1986年绘制的图表进行了更新，展示出许多常见单变量概率分布的属性和关联。\n\n\n\n图-1\n\n\n图-1 含有 76 个单变量概率分布，其中 19 个离散分布，57 个连续分布。矩形外框表示离散分布，圆角外框表示连续分布。除 Benford 分布外，离散分布都排布在图表上部。每个分布包含两行描述。第一行给出分布的名称和参数，第二行列举其属性。\n对分布参数做如下约定：\n\nn: 无论有无下标，表示正整数\np: 满足 0 &lt; p &lt; 1\n\\alpha, \\sigma: 无论有无下标，表示正的尺度参数 (scale parameter)\n\\beta, \\gamma, \\kappa: 表示正的形状参数 (shape parameter)\n\\mu, a, b: 表示位置参数 (location parameter)\n\\lambda, \\delta: 表示正的参数\n\n偶尔的例外会在附录中注明 (附录见原文)。 受限于排版，有好多分布没有被包含在图中。"
  },
  {
    "objectID": "posts/univariate_distribution_relationships/index.html#引言",
    "href": "posts/univariate_distribution_relationships/index.html#引言",
    "title": "[译注] 单变量概率分布之间的关系",
    "section": "",
    "text": "在传统的数理统计教科书中，概率分布往往被孤立的介绍，很少讨论他们之间的联系。本文对 Leemis 在1986年绘制的图表进行了更新，展示出许多常见单变量概率分布的属性和关联。\n\n\n\n图-1\n\n\n图-1 含有 76 个单变量概率分布，其中 19 个离散分布，57 个连续分布。矩形外框表示离散分布，圆角外框表示连续分布。除 Benford 分布外，离散分布都排布在图表上部。每个分布包含两行描述。第一行给出分布的名称和参数，第二行列举其属性。\n对分布参数做如下约定：\n\nn: 无论有无下标，表示正整数\np: 满足 0 &lt; p &lt; 1\n\\alpha, \\sigma: 无论有无下标，表示正的尺度参数 (scale parameter)\n\\beta, \\gamma, \\kappa: 表示正的形状参数 (shape parameter)\n\\mu, a, b: 表示位置参数 (location parameter)\n\\lambda, \\delta: 表示正的参数\n\n偶尔的例外会在附录中注明 (附录见原文)。 受限于排版，有好多分布没有被包含在图中。"
  },
  {
    "objectID": "posts/univariate_distribution_relationships/index.html#分布属性",
    "href": "posts/univariate_distribution_relationships/index.html#分布属性",
    "title": "[译注] 单变量概率分布之间的关系",
    "section": "分布属性",
    "text": "分布属性\n单变量概率分布有不同的属性，列在 图-1 中。\n\nLinear combination (线性组合性)：表明服从此分布的多个独立随机变量的线性组合服仍从此分布。\n例：若 n 个独立随机变量 X_i \\sim N(\\mu_i, \\sigma_i^2)，其中 i=1,2,...,n 且 a_i \\in \\R，则\n\\sum_{i=1}^n a_i X_i \\sim N \\bigg( \\sum_{i=1}^n a_i \\mu_i, \\sum_{i=1}^n a_i^2 \\sigma_i^2 \\bigg)\n证明-1\nConvolution (卷积性)：表明服从此分布的多个独立随机变量之和仍服从此分布。\n例：若 n 个独立随机变量 X_i \\sim \\chi^2(k_i)，其中 i=1,2,...,n，则\n\\sum_{i=1}^n X_i \\sim \\chi^2\\bigg(\\sum_{i=1}^n k_i \\bigg)\n证明-2\nScaling (缩放性)：表明服从此分布的独立随机变量乘以任意正实数后仍服从此分布。\n例：若随机变量 X \\sim \\text{Weibull}(\\alpha, \\beta) 且 k&gt;0，则\n kX \\sim \\text{Weibull}(\\alpha k ^\\beta, \\beta)\n证明-3\nProduct (乘积性)：表明服从此分布的独立随机变量之积仍服从此分布。\n例：若 n 个独立随机变量 X_i \\sim \\text{Lognormal}(\\mu_i, \\sigma_i^2)，其中 i=1,2,...,n，则\n\\prod_{i=1}^n X_i \\sim \\text{Lognormal}\\bigg(\\sum_{i=1}^n \\mu_i, \\sum_{i=1}^n \\sigma_i^2 \\bigg)\n证明-4\nInverse (倒数性)：表明服从此分布的随机变量的倒数仍服从此分布。\n例：若随机变量 X \\sim F(n_1, n_2) 则\n\\frac 1 X \\sim F(n_2, n_1)\n注：服从 F distribution\n证明-5\nMinimum (最小性)：表明多个服从此分布的独立随机变量的最小值仍服从此分布。\n例：若 n 个独立随机变量 X_i \\sim \\text{Exp}(\\alpha_i)，其中 i=1,2,...,n，则\n\\min\\{X_1, X_2,...,X_n\\} \\sim \\text{Exp} \\bigg(1 \\bigg/ \\sum_{i=1}^n \\frac 1 {\\alpha_i} \\bigg)\n证明-6\nMaXimum (最大性)：表明多个服从此分布的独立随机变量的最大值仍服从此分布。\n例：若 n 个独立随机变量 X_i \\sim \\text{standard power}(\\beta_i)，其中 i=1,2,...,n，则\n\\max\\{X_1, X_2,...,X_n\\} \\sim \\text{standard power} \\bigg(\\sum_{i=1}^n \\beta_i \\bigg)\n证明-7\nForgetfulness (健忘性)：也叫做无记忆性，表明此分布的随机变量大于某值的条件分布与原无条件分布相同，即 X|X&gt;a 与 X 的分布不仅同族且分布参数也相同。\n例：离散分布中只有 Geometric 分布，连续分布中只有 Exponential 分布具有这种性质。\n证明-8\nResidual (残留性)：表明此分布的随机变量大于某值的条件分布与原无条件分布同族，即 X|X&gt;a 与 X 服从同一类分布，但分布参数可以不同。可见健忘性是残留性的特例。\n例：若 X \\sim \\text{ Uniform }(a,b)，而 k \\in (a,b)，则 X&gt;k|X 也属于均匀分布。\n证明-9\n\n\n\n\n\n\n注记\n\n\n\n健忘性 和 残留性 一般都是针对 X&gt;a 这种形式的条件进行定义的。因为应用中常关注等待时间或生存时间超过某一阈值后的情况。如果硬要采用其他条件，那就要具体情况具体分析了。比如对于 指数分布，如果以 X&lt;a 为条件，就不是指数分布了。\n\n\nVariate generation (变量生成性)：表明连续随机变量的反累计分布函数 (Inverse CDF) 可以表达为封闭形式 (closed form)。对于离散随机变量，具有这个属性意味着存在时间复杂度为 O(1) 的算法来生成这个随机变量，无需循环或利用其他特性。\n例：若随机变量 X \\sim \\text{Exp}(\\alpha)，则其 Inverse CDF 为\nF^{-1}(u) = -\\alpha \\ln(1-u), \\; u \\in (0,1)\n证明-10\n\n这些属性之间存在包含关系。属性 L 包含了属性 C 和 S，所以图中 C 和 S 不会被出现在具有 L 属性的分布中。类似的还有属性 F 包含着 R。\n有些属性只在特定情况下才成立。比如具有的 M 属性的 Weibull 分布，只有当形状参数固定时才成立。所以在图-1中 Weibull 分布的第二行写成 M_\\beta，以示限制。"
  },
  {
    "objectID": "posts/univariate_distribution_relationships/index.html#分布之间的关系",
    "href": "posts/univariate_distribution_relationships/index.html#分布之间的关系",
    "title": "[译注] 单变量概率分布之间的关系",
    "section": "分布之间的关系",
    "text": "分布之间的关系\n图-1 使用了三种线型来连接分布：\n\n实线 表示特殊情形 (Special cases) 和 变换 (Transformations)。变换的标注中通常出现 X，以区别于特殊情形。术语 “变换” 在这里使用得很宽泛，包括 顺序统计量 (Order statistic) 的分布，截断随机变量，或采用随机变量的混合。\n虚线 表示渐进关系。通常是对一个或多个参数向参数空间的边界取极限。\n点线 表示贝叶斯关系。例如 Beta-binomial，Beta-Pascal，Gamma-normal，Gamma-Poisson。证明-11\n\nBinomial、\\chi^2、Exponential、Gamma、Normal 和 Uniform分布作为枢纽出现，突出了它们在应用统计中的中心地位。 求和限制在从 i=1 到 n。符号 X_{(r)} 表示从大小为 n 的随机样本中抽取的第 r 个顺序统计量。\n不同分布在合适的分布参数下会成为同一分布。例如：证明-12\n\n均值为 2 的 Exponential 分布 和 2个自由度的 \\chi^2 分布。\n具有偶数自由度的 \\chi^2 分布 与 尺度参数为 2 的 Erlang 分布。\n样本大小 n=1 的 Kolmogorov-Smirnov 分布 与 (1/2, 1) 的 Uniform 分布。\n\n这些关系都用双头箭头表示。\n利用概率积分变换，可以在 Standard uniform 分布和所有满足 F(X) \\sim \\text{Uniform}(0,1) 的分布之间建立联系。类似的，可以在 Standard exponential 分布和所有满足 H(X) \\sim \\text{Exp}(1) 的分布之间建立联系。其中 H(x)=\\int_{-\\infin}^x f(t)/(1-F(t))dt 是累计风险函数 (Cumulative hazard function)。证明-13\n所有可以表示为随机变量之和的随机变量，在求和项数趋于无穷时，由于中心极限定理，其分布逐渐收敛到 Normal 分布。这类分布有：\n\nBinomial 分布：n 个独立的 Bernoulli 随机变量之和。每次实验的成功概率为 p。描述 n 次独立 Bernoulli 实验中，成功的次数。\n\\chi^2 分布：k 个独立的 Standard normal 随机变量的平方和的分布，其中k是自由度。\nErlang 分布：k 个独立同分布的 Exponential 随机变量之和，其中 k 是正整数。描述等待收到 k 次信号的时间间隔。\nGamma 分布：k 个独立同分布的 Exponential 随机变量之和，其中 k 是正实数。Gamma 分布是 Erlang 分布的推广。描述完成 k 的任务量(可以是部分完成)所需的时间。\nHypoexponential 分布：类似于 Gamma 分布，但可以是独立的多个不同 Exponential 随机变量之和，每个 Exponential 分布都有自己的参数和权重。用于描述多阶段系统的总完成时间。\nPascal 分布：n 个独立的 Bernoulli 随机变量之和。描述需要累计到 k 次成功所需的实验次数。\n\n此外，所有分布都跟 Normal 分布有渐进 (Asymptotic) 关系，只要他们具有有限均值和有限方差，中心极限定理保证了随机变量的和收敛到 Normal 分布。\n许多变换是可逆的，图-1中以双箭头连接两个分布。考虑 Normal 分布和 Standard normal 分布的关系。如果 X \\sim N(\\mu, \\sigma^2)，则 \\frac {X-\\mu}{\\sigma} \\sim N(0,1)。反过来，如果 X \\sim N(0,1)，则 \\mu+\\sigma X \\sim N(\\mu, \\sigma^2)。前一个变换用于将随机变量标准化，以便查表求值。后一个变换用于随机变量的生成。但为了简洁， 在大多数情况下逆变换是隐含的，并不显示在图中。比如对 Weibull 随机变量取对数得到 Extreme value 随机变量；对 Extreme value 随机变量取指数得到 Weibull随机变量。证明-14\n有些变换可以产生尚未定名的全新分布： 证明-15\n\nExtreme value 分布和 Log gamma 分布意味着对任意 Survival distribution 取对数的变换，都将得到一个Support是整个实数轴的分布。\nInverted gamma distribution 意味着对任意 Survival distribution 取倒数可以得到另一个 Survival distribution。\n对于Support为 (0,1) 的随机变量，交换 F(x), F^{-1}(u) 的角色会得到对应的 Complementary distribution。\n\n另外，图-1中的一些变换可以启发随机数生成器的设计。比如 Box-Muller 算法，将 \\text{Uniform}(0,1) 先变换到指数分布，再到 \\chi^2分布，再到 Standard normal 分布，最后利用缩放变换到所需的 Normal 分布。 证明-16\n多余的箭头一般不会出现在图中。比如 Minimax 分布和 Standard Uniform 分布之间并没有箭头连接，因为已经有 \\text{Minimax} \\rightarrow \\text{Standard power} \\rightarrow \\text{Uniform}(0,1)。类似的，虽然 Exponential 分布是 Gamma 分布的特殊情形，但二者没有箭头连接，因为已有 \\text{Gamma} \\rightarrow \\text{Erlang} \\rightarrow \\text{Exponential}。\n为了保持图表清晰，有些不适合或相距太远的分布并没出现在其中。比如：\n\nExponential 随机变量的向下取整，得到 Geometric 随机变量。 证明-17\n连续 Uniform 随机变量向下取整，得到离散 Uniform 随机变量。\nExponential 随机变量是 Makeham 随机变量在 \\delta=0 的特殊情况。\nStandard power 随机变量是 Beta 随机变量在 \\delta=1 的特殊情况。\n如果 X \\sim F(n_1, n_2), 则 \\frac 1 {1+(n_1/n_2)X} 服从 Beta 分布。 证明-18\n以 n_1, n_2 为自由度, 以 \\delta,\\gamma 为非中心参数的双重非中心 F 分布定义为 \\big( \\frac {X_1(\\delta)}{n_1} \\big)\\big( \\frac {X_2(\\gamma)}{n_2} \\big)^{-1}， 其中 X_1(\\delta),X_2(\\gamma) 是非中心 \\chi^2 随机变量，自由度分别是 {n_1},{n_2}。 wikipedia\nNormal 分布和 Uniform 分布分别是 Generalized error 分布的特例和极限情况。 wikipedia\nBinomial 分布是 Power series 分布的特例。 证明-19\nNormal 分布是 von Mises 分布在 \\kappa \\to \\infin 时的特例。 证明-20\nHalf-normal，Rayleigh，Maxwell-Boltzmann 分布是 \\chi 分布在自由度分别为 n=1,2,3 的情形。 证明-21\n两个独立的 Generalized gamma 随机变量的比值是 Beta 分布。 证明-22\n\n此外，还有一些没绘制在图表中变换，是由两种分布组合来得到第三种分布。以下是两个例子:\n\n自由度 n 的 t 分布可定义为：\\frac Z {\\sqrt{X/n}}, 其中 Z \\sim N(0,1), X \\sim \\chi^2(n)，且二者独立。 证明-23\n以 \\delta 为非中心参数的非中心Beta分布可以定义为：\\frac X {X+Y}，其中 X \\sim \\text{noncentral } \\chi^2(\\beta,\\delta), Y \\sim \\chi^2(\\gamma)。 证明-24\n\n\n\n证明-1\n多个随机变量的线性组合所服从的分布不是多个分布函数的简单叠加。比如计算掷两次骰子的点数之和的分布，和为 5 点的概率等于 \\{(1, 4), (2, 3), (3, 2), (4, 1)\\} 四种情况的概率之和，也就是需要计算卷积。\n考虑随机变量 Y = a_1 X_1 + a_2 X_2 的简单情形，其中随机变量 X_1, X_2 独立。将 X_1 视为自由的变量，则对于给定的 Y，有 X_2=\\frac {W - a_1 X_1}{a_2}。于是 Y 的概率密度函数 (PDF) 应为:\nf_Y(y)  = \\int f_{X_1, X_2}(x_1, \\frac {y - a_1 x_1}{a_2}) dx_1 = \\int f_{X_1}(x_1)f_{X_2}(\\frac {y - a_1 x_1}{a_2}) dx_1\n但这个积分比较难算，特别是当涉及多个随机变量时。于是考虑使用特征函数 (Characteristic function)，它对于计算多个随机变量的线性组合特别有用。特征函数是对概率分布的傅里叶变换。特征函数定义为:\n\\varphi_X(t) \\colonequals E[e^{itX}]=\\int e^{itx} dF_X(x)=\\int e^{itx} f_X(x)dx\n其中 E 表示期望值，F_X, f_X 分别是随机变量 X 的累计分布函数 (CDF) 和 概率密度函数 (PDF).\n由于 X_1, X_2 独立，所以期望值可以分解：\n\n\\begin{align*}\n\\varphi_Y(t) &= E[e^{itY}] \\\\\n&= \\int \\int e^{it(a_1 x_1 + a_2 x_2)} f_{X_1,X_2}(x_1,x_2) dx_1 dx_2  \\\\\n&= \\int e^{i a_1 t x_1} f_{X_1}(x_1) dx_1 \\int e^{i a_2 t x_2} f_{X_2}(x_2) dx_2 \\\\\n&= E[e^{i a_1 t x_1}]E[e^{i a_2 t x_2}]  \\\\\n&= \\varphi_{X_1}(a_1 t)\\varphi_{X_2}(a_2 t)\n\\end{align*}\n\n同理可以推广到多个随机变量的线性组合：\n\\varphi_{a_1 X_1+...+a_n X_n}(t)=\\varphi_{X_1}(a_1 t)...\\varphi_{X_n}(a_n t)\n设 随机变量 Y=\\sum_{i=1}^n a_i X_i，其中 X_i 是 n 个独立的 Normal 随机变量 X_i \\sim N(\\mu_i, \\sigma_i^2),\\; i=1,2,...,n。已知 Normal 分布的特征函数为：\n\\varphi_X(t)=e^{i\\mu t - \\frac 1 2 \\sigma^2 t^2}\n代入得到\n\n\\begin{align*}\n\\varphi_Y(t) &= e^{i (a_1 t)\\mu_1 - \\frac 1 2 \\sigma_1^2 (a_1 t)^2} ... e^{i (a_n t)\\mu_n - \\frac 1 2 \\sigma_n^2 (a_n t)^2} \\\\\n&= e^{i(a_1 \\mu_1 +...+a_n \\mu_n)t - \\frac 1 2 (a_1^2\\sigma_1^2+...+a_n^2\\sigma_n^2) t^2} \\\\\n&= e^{i \\hat\\mu t - \\frac 1 2 {\\hat \\sigma}^2 t^2}\n\\end{align*}\n\n对比可知\nY \\sim N(\\hat \\mu, {\\hat \\sigma}^2)=N(\\sum_{i=1}^n a_i \\mu_i, \\sum_{i=1}^n a_i^2 \\sigma_i^2)\n\\square\n\n\n\n证明-2\n涉及多个随机变量求和，类似于证明-1，还是使用特征函数。\n令随机变量 Y=\\sum_{i=1}^n X_i，其中 X_i \\sim \\chi^2(k_i),\\; i=1,2,...,n 是独立的服从 \\chi^2 分布的随机变量。\n已知 \\chi^2 分布的特征函数为\n\\varphi_{X}(t)=(1-2it)^{-k/2}\n所以\n\\varphi_Y(t)=\\sum_{i=1}^n \\varphi_{X_i}(t)=(1-2it)^{-\\sum_{i=1}^n k_i/2}=(1-2it)^{-\\hat k / 2}\n对比可知\nY \\sim \\chi^2(\\hat k)=\\chi^2\\bigg(\\sum_{i=1}^n k_i \\bigg)\n\\square\n\n\n\n证明-3\n只是简单的随机变量变换。设随机变量 Y=kX, \\; k&gt;0，则 Y 的累计分布函数为：\nF_Y(y) \\colonequals \\Pr(Y \\le y)=\\Pr(X \\le y/k)=\\int_{-\\infin}^{y/k} f_X(x)dx\n所以 Y 的概率密度函数为\nf_Y(y) \\colonequals \\frac d {dy} F_Y(y) = \\frac 1 k f_X(y/k)\n若随机变量 X \\sim \\text{Weibull}(\\alpha, \\beta)，由定义其概率密度为：\nf_X(x) =\\frac \\beta \\alpha x^{\\beta - 1} \\exp(-\\frac 1 {\\alpha} x^\\beta)\n代入得\nf_Y(y) = \\frac \\beta {\\alpha k^\\beta} y^{\\beta - 1} \\exp(-\\frac 1 {\\alpha k^\\beta} y^\\beta)\n对比可知\nY \\sim \\text{Weibull}(\\alpha k^\\beta, \\beta)\n\\square\n\n\n\n证明-4\n可以利用 Lognormal 分布取对数得到 Normal 分布这个性质来证明。\n设随机变量 X_1 \\sim \\text{Lognormal}(\\mu_1, \\sigma_1^2), X_2 \\sim \\text{Lognormal}(\\mu_2, \\sigma_2^2)，则随机变量 Y_1 = \\ln(X_1) \\sim N(\\mu_1, \\sigma_1^2), Y_2 = \\ln(X_2) \\sim N(\\mu_2, \\sigma_2^2)。\n令随机变量 Z=X_1 X_2, W= \\ln(Z) = \\ln(X_1) + \\ln(X_2) = Y_1 + Y_2。由 证明-1 可知, W \\sim N(\\mu_1+\\mu_2, \\sigma_1^2+\\sigma_2^2)，所以 Z \\sim \\text{Lognormal}(\\mu_1+\\mu_2, \\sigma_1^2+\\sigma_2^2)。同理可以推广到多个随机变量的情形。\n\\square\n\n\n\n证明-5\n可以利用 F 分布是两个 \\chi^2 分布的比值这个性质来证明。\n设随机变量 U \\sim \\chi^2(d_1), V \\sim \\chi^2(d_2)，且二者独立，则随机变量 X=\\frac U {d_1} \\big/ \\frac V {d_2} \\sim F(d_1, d_2)，所以 1/X = \\frac V {d_2} \\big/ \\frac U {d_1} \\sim F(d_2, d1) 。\n\\square\n\n\n\n证明-6\n设随机变量 Y=\\min(X_1, X_2)，其中 X_1 \\sim \\text{Exp}(\\alpha_1), X_2 \\sim \\text{Exp}(\\alpha_2)，且 X_1, X_2 独立，则 Y 的累计分布函数为：\n\n\\begin{align*}\nF_Y(y) &\\colonequals \\Pr(Y \\le y) \\\\\n&=1-\\Pr(Y&gt;y) \\\\\n&=1-\\Pr(X_1&gt;y, X_2&gt;y) \\\\\n&=1-\\Pr(X_1&gt;y) \\Pr(X_2&gt;y)\n\\end{align*}\n\n由定义，若随机变量 X \\sim \\text{Exp}(\\alpha)，则其概率密度函数为：\nf_X(x)=\\frac 1 \\alpha e^{-\\frac x \\alpha}, \\; x&gt;0\n代入得\nF_Y(y)=1-\\int_y^\\infin\\frac 1 {\\alpha_1} e^{-\\frac {x_1} {\\alpha_1} } dx_1 \\int_y^\\infin\\frac 1 {\\alpha_2} e^{-\\frac {x_2} {\\alpha_2} } dx_2=1-e^{-\\frac y {\\alpha_1}}e^{-\\frac y {\\alpha_2}}\n于是 Y 的概率密度函数为\nf_Y(y) \\colonequals \\frac d {dy} F_Y(y)=(\\frac 1 {\\alpha_1} + \\frac 1 {\\alpha_2}) e^{-y(\\frac 1 {\\alpha_1} + \\frac 1 {\\alpha_2})} = \\frac 1 {\\hat \\alpha} e^{-\\frac x {\\hat \\alpha}}\n对比可知\nY \\sim \\text{Exp}(\\hat \\alpha)\n同理可以推广到多个随机变量的情形。\n\\square\n\n\n\n证明-7\n设随机变量 Y=\\max(X_1, X_2)，其中 X_1 \\sim \\text{standard power}(\\beta_1), X_2 \\sim \\text{standard power}(\\beta_2)，且 X_1, X_2 独立，则 Y 的累计分布函数为\n\n\\begin{align*}\nF_Y(y) &\\colonequals \\Pr(Y \\le y) \\\\\n&=\\Pr(X_1 \\le y, X_2 \\le y) \\\\\n&=\\Pr(X_1 \\le y) \\Pr(X_2 \\le y)\n\\end{align*}\n\n由定义，若随机变量 X \\sim \\text{standard power}(\\beta)，则其概率密度函数为\nf_X(x)=\\beta x^{\\beta - 1}, \\; x\\in(0,1)\n代入得\nF_Y(y)=\\int_0^y \\beta_1 x_1^{\\beta_1 - 1} dx_1 \\int_0^y \\beta_2 x_2^{\\beta_2 - 1} dx_2=y^{\\beta_1}y^{\\beta_2}\n于是 Y 的概率密度函数为\nf_Y(y) \\colonequals \\frac d {dy} F_Y(y)=(\\beta_1 + \\beta_2) y^{\\beta_1 + \\beta_2 - 1}=\\hat \\beta y^{\\hat \\beta - 1}\n对比可知\nY \\sim \\text{standard power}(\\beta_1+\\beta_2)\n同理可以推广到多个随机变量的情形。\n\\square\n\n\n\n证明-8\nExponential 分布通常用于描述事件首次出现所需等待的时间。若随机变量 X \\sim \\text{Exp}(1 / \\lambda)，令 Y=X|X&gt;a,\\; a&gt;0 是 X 被限制在 X&gt;a 条件下的随机变量，其概率密度函数为：\nf_Y(y)=\\frac {f_X(y)} {\\Pr(X&gt;a)} = \\frac {\\lambda e^{-\\lambda y}} {e^{-\\lambda a}}=\\lambda e^{-\\lambda(y-a)} \\text{ for } y&gt;a\n对比 Exponential 分布的概率密度函数：\nf_X(x)=\\lambda e^{-\\lambda x}\n可见这个分布只是平移了 a 而已，分布和参数都不变。容易验证：\n\\Pr(Y&gt;b) = \\Pr(X &gt; b | X &gt; a) = \\Pr(X &gt; b-a) \\; \\text{ for } b&gt;a\n这个等式的含义是，假如已知从 0 时刻到 a 时刻，公交车没到站的话 (即 X&gt;a)，那么继续等待到 b 时刻，车还没到的概率 与 无条件时 (即从 0 时刻开始) 等待 b-a 时间后，车还没到的概率相同。换言之，无论之前已经等了多久，公交车到站时间的概率分布不变，就好像之前从未等待过一样。\n\\square\nGeometric 分布用于描述连续的Bernoulli实验中，要获得首次成功所需的实验次数。这个分布有两种稍有不同的定义方式：\n\n\\Pr(X=k)=p(1-p)^{k-1}, \\; k=1,2,3,...\n\\Pr(X'=k)=p(1-p)^k=\\Pr(X=k+1), \\; k=0,1,2,...\n\n其中 p 是 Bernouli 实验成功的概率。先考虑第一种定义。随机变量 X 表示经过 X 次实验获得首次成功，也就是经历连续失败 X-1 次外加最后成功的一次。\n\n\\begin{align*}\n\\Pr(X|X&gt;n) &= \\frac {\\Pr(X)} {\\Pr(X&gt;n)} \\\\\n&= \\frac {\\Pr(X)} {1 - \\Pr(X \\le n)} \\\\\n&= \\frac {p(1-p)^{x-1}} {1 - \\sum_{i=1}^{n} p(1-p)^{i-1}} \\\\\n&= p(1-p)^{x-n-1} \\\\\n&= \\Pr(X-n)\n\\end{align*}\n\n即 \\Pr(X|X&gt;n)=\\Pr(X-n)。其中的 X-n 意味着扣除那 n 次失败，重新计数。换成白话就是，无论之前已经努力过多少次，承受了多少失败，距离成功还像当初一样遥远，就好像之前从未努力过一样。这种无记忆性也可以写成 \\Pr(X&gt;m+n | X&gt;n)=\\Pr(X&gt;m)，对任意的 m,n 都成立。注意等式右侧与 n 无关，表示忘记历史。\n如果考虑 Geometric 分布的第二种定义，\n\n\\begin{align*}\n\\Pr(X'|X'&gt;n) &= \\frac {\\Pr(X')} {\\Pr(X'&gt;n)} \\\\\n&= \\frac {\\Pr(X')} {1 - \\Pr(X' \\le n)} \\\\\n&= \\frac {p(1-p)^x} {1 - \\sum_{i=0}^n p(1-p)^i} \\\\\n&= p(1-p)^{x-n-1} \\\\\n&= \\Pr(X'-n-1)\n\\end{align*}\n\n即 \\Pr(X'|X'&gt;n)=\\Pr(X'-n-1)，同理得 \\Pr(X'&gt;m+n|X'&gt;n)=\\Pr(X'&gt;m-1)，对任意的 m,n 都成立。这里虽然也体现了与历史 n 的无关性，但由于多了一个 -1，并非标准定义下的无记忆性，所以 Geometric 分布的第二种定义不具备无记忆性。\n\\square\n\n\n\n证明-9\n若随机变量 X \\sim \\text{Uniform}(a,b), 令 Y=X|X&gt;k,\\; k\\in(a,b) 是X被限制在大于k的条件下的随机变量. 其概率密度函数为:\nf_Y(y)=\\frac {f_X(y)} {\\Pr(X&gt;k)} = \\frac {\\frac 1 {b-a}} {\\int_k^b \\frac 1 {b-a}dx}=\\frac 1 {b-k}, \\; \\text {for } y\\in(k,b)\n对比均匀分布的概率密度函数:\nf_X(x)=\\frac 1 {b-a}, \\; x\\in(a,b)\n可见 Y \\sim \\text{Uniform}(k, b), 但参数变化了.\n\\square\n\n\n\n证明-10\n若随机变量 X \\sim \\text{Exp}(\\alpha), 则累计分布函数为\nF_X(x)=\\int_0^x f_X(t)dt=\\int_0^x \\frac 1 \\alpha e^{-\\frac t \\alpha} dt=1-e^{-\\frac x \\alpha}, \\; x \\ge 0\nF_X(x) 的定义域为 x\\in [0,\\infin), 值域为 F_X \\in [0, 1], 且是递增函数, 所以有反函数\ng(u)=F_X^{-1}(u) = -\\alpha \\ln(1-u), \\; u \\in [0, 1]\ng(u) 是closed form这个性质可以用于随机数生成算法. 比如从均匀分布 Y \\sim \\text{Uniform}(0,1) 的随机变量来构造一个服从指数分布的随机变量. 只需用 Inverse CDF 对 Y 做变换即可. 即\nZ = g(Y) \\sim \\text{Exp}(\\alpha)\n为了验证这一点, 可以计算 Z 的累计分布函数, 并注意到 g(u) 是递增函数, 有:\nF_Z(z)=\\Pr(g(Y) \\le g(z))=\\Pr(Y \\le g^{-1}(z))=\\int_0^{g^{-1}(z)} 1 dt=g^{-1}(z)=F_X(z)\n可见 X 与 Z 具有同样的累计分布函数, 即服从同样的分布. 计算机只需把 Y 的每个采样结果经过 Z=g(Y) 这个简单的变换后, 就能以 O(1) 的时间复杂度来得到服从指数分布的新随机数 Z .\n\\square\n\n\n\n证明-11\nBeta-binomial分布是一种离散的复合分布. 它以Binomial分布为基础, 但分布参数 p 不是固定值, 而是从 Beta 分布中抽取而来. 整个分布要以 Beta分布 的概率为权重遍历所有的参数 p. 其PMF为:\n\n\\begin{align*}\nf(x|n,a,b)&=\\int_0^1 \\text{Bin}(x|n,p) \\text{Beta}(p,a,b) dp \\\\\n&= {n \\choose x} \\frac 1 {B(a,b)} \\int_0^1 p^{x+a-1}(1-p)^{n-x+b-1} dp \\\\\n&= {n \\choose x} \\frac {B(x+a, n-x+b)} {B(a,b)} \\\\\n&= \\frac {\\Gamma(n+1)} {\\Gamma(x+1)\\Gamma(n-x+1)} \\frac {\\Gamma(x+a)\\Gamma(n-x+b)}{\\Gamma(n+a+b)} \\frac {\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)} \\\\\n\\text {where } x = 0,1,...,n\n\\end{align*}\n\n其中 B(a,b) 是 Beta function:\nB(a,b)=\\frac {\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)} = \\int_0^1 x^{a-1}(1-x)^{b-1}dx\nPascal分布也叫负二项分布 (negative binomial distribution), 主要描述在一系列独立同分布的伯努利实验中, 达到指定的成功次数所需进行的实验次数。这与几何分布有相似之处, 不过几何分布只关心出现第一次成功所需的实验次数. Beta-Pascal分布也是一种离散的复合分布. 它以Pascal分布为基础, 但分布参数 p 不是固定值, 而是从 Beta 分布中抽取而来. 整个分布要以 Beta分布 的概率为权重遍历所有的参数 p.\nGamma分布是一种连续概率分布, 主要用于描述等待k个独立的事件发生所需的时间, 假设这些事件以恒定的平均速率随机地发生. Gamma-Poisson 分布是一种离散的复合分布. 它以Poisson分布为基础, 但分布参数 \\mu 服从Gamma 分布. 其PMF为:\n\n\\begin{align*}\nf(x|\\alpha,\\beta) &= \\int_0^\\infin \\frac {\\mu^x e^{-\\mu}}{x!} \\frac 1 {\\alpha^\\beta \\Gamma(\\beta)} \\mu^{\\beta-1} e^{-\\mu / \\alpha} d\\mu\n\\end{align*}\n\n由于 \\Gamma(c)=\\int_0^\\infin x^{c-1} e^{-x} dx, 令 t=\\mu(1+1/\\alpha), 代入上式\n\n\\begin{align*}\nf(x|\\alpha,\\beta) &= \\frac {1} {x! \\alpha^\\beta \\Gamma(\\beta)} \\int_0^\\infin \\mu^{x+\\beta-1} e^{-\\mu(1+1/\\alpha)} d\\mu \\\\\n&= \\frac {1} {x! \\alpha^\\beta \\Gamma(\\beta)} \\int_0^\\infin (\\frac t {1+1/\\alpha})^{x+\\beta-1} e^{-t} \\frac 1 {1+1/\\alpha} dt \\\\\n&= \\frac {1} {x! \\alpha^\\beta \\Gamma(\\beta)} (\\frac 1 {1 + 1/\\alpha})^{x+\\beta} \\int_0^\\infin t^{x+\\beta-1} e^{-t} dt \\\\\n&= \\frac {\\Gamma(x+\\beta) \\alpha^x} {\\Gamma(\\beta) (1+\\alpha)^{x+\\beta} x! } \\\\\n\\text{where } x=0,1,...\n\\end{align*}\n\n\\square\n\n\n\n证明-12\n\n指数分布的均值为2, 意味着分布参数 \\alpha=2. \\chi^2 分布的自由度为2, 意味着分布参数 n=2. 此时两种分布的PDF均为: f(x)=\\frac 1 2 e^{-x/2}, \\; x&gt;0.\n自由度 n=2k, \\;k=1,2,... 的 \\chi^2 分布 和 尺度参数 \\alpha=2 的 Erlang分布的PDF均为: f(x)=\\frac 1 {2^k (k-1)!}x^{k-1} e^{-x/2}, \\; x&gt;0.\n样本大小 n=1 的Kolmogorov-Smirnov分布的 CDF 为: \nF_{D_1}(t)=\\Pr(D_1 \\le t)=\\begin{cases}\n0 & t \\le \\frac 1 2 \\\\\n2t-1 & \\frac 1 2 &lt; t &lt; 1 \\\\\n1 & t \\ge 1\n\\end{cases}\n 而 \\text{Uniform}(\\frac 1 2, 1) 的 CDF 为: \nF_X(x)=\\begin{cases}\n0 & x \\le \\frac 1 2 \\\\\n\\int_{\\frac 1 2}^x 2 du= 2x-1 & \\frac 1 2 &lt; x &lt; 1 \\\\\n1 & x \\ge 1\n\\end{cases}\n 二者一致.\n\n\\square\n\n\n\n证明-13\n对于积分变换 F(X) \\sim \\text{Uniform}(0,1), 其实就是 证明-10 的一般情况. 也就是通过 Inverse CDF (也称分位函数 quantile function), 把标准均匀分布变换为任意一种分布. 当然, 此处的 Inverse CDF 不一定具有 closed form 了.\n设随机变量 X \\sim \\text{Uniform}(0,1). 随机变量 Y 的 CDF 为 F(y). 显然其 Inverse CDF: F^{-1}(p) 的定义域是 [0, 1]. 用这个函数将随机变量 X 变换为 Z=F^{-1}(X). 下面只要证明 Z 的 CDF F_Z(z) 就是 Y 的 CDF 即可.\n\n\\begin{align*}\nF_Z(z) &= \\Pr(Z \\le z) \\\\\n&= \\Pr(F^{-1}(X) \\le z) \\\\\n&= \\Pr(X \\le F(z)) \\\\\n&= \\int_0^{F(z)} 1 dt = F(z)\n\\end{align*}\n\n对于积分变换 H(X) \\sim \\text{Exp}(1). 可以利用 累计风险函数 和 生存函数 的关系来证明. - 生存函数定义为 S(t) = 1 - F(t)=\\Pr(T&gt;t), 表示等待到 t 时刻还没发生事故的概率. - 风险函数 h(t)=\\frac {f(t)}{S(t)}=-\\frac d {dt} \\ln S(t), 表示获知 T \\le t 都不会发生事故的前提下, t 时刻发生事故的概率(或瞬时风险), 即 \\Pr(T=t|T&gt;t). - 累计风险函数 H(t) = \\int_{-\\infin}^t h(u)du = -\\ln S(t) 就是累计到 t 时刻的总风险. 值域是 [0, \\infin] .\n所以 e^{-H(t)} = S(t) = 1 - F(t). 进而利用 Inverse CDF 得到 t = F^{-1}(1 - e^{-H(t)}). 这正是累计风险函数的反函数 H^{-1}(p)=F^{-1}(1-e^{-p}). 设随机变量 T 的 CDF 为 F(t). 用 H(t) 将 T 变换为 U=H(T). 下面证明 U \\sim \\text{Exp}(1), 也就是 U 的 CDF F_U(u) = 1 - e^{-u}.\n\n\\begin{align*}\nF_U(u) &= \\Pr(U \\le u) \\\\\n&= \\Pr(H(T) \\le u) \\\\\n&= \\Pr(T \\le H^{-1}(u)) \\\\\n&= F(H^{-1}(u)) \\\\\n&= F(F^{-1}(1-e^{-u})) \\\\\n&= 1 - e^{-u}\n\\end{align*}\n\n\\square\n\n\n\n证明-14\n设随机变量 X \\sim \\text{Weibull}(\\alpha, \\beta), 其PDF:\nf_X(x)=\\frac \\beta \\alpha x^{\\beta-1} e^{-x^\\beta / \\alpha}, \\; x&gt;0\n令随机变量 Y=\\log X. 于是:\n\\Pr(Y&lt;y)=\\Pr(\\log X &lt; y)=\\Pr(X &lt; e^y)\n所以 Y 的PDF为:\n\n\\begin{align*}\nf_Y(y) &=\\frac d {dy} F_X(e^y) \\\\\n&= f_X(e^y) e^y \\\\\n&= \\frac \\beta \\alpha e^{y(\\beta-1)} e^{-e^{\\beta y} / \\alpha} e^y \\\\\n&= \\frac \\beta \\alpha e^{y \\beta - e^{y \\beta} / \\alpha} \\\\\n\\text{where } y \\in \\R\n\\end{align*}\n\n所以 Y \\sim \\text{Extreme value}(\\alpha, \\beta). 同理可证逆变换.\n\\square\n\n\n\n证明-15\nSurvival distribution是一个分布族. 其特点是support为 T \\ge 0, 且 T 必须表示 time to event, 比如设备故障, 生物死亡的时间. 常见的 Survival distribution 有:\n\nExponential distribution\nWeibull distribution\nLognormal distribution\nGamma distribution\n\n服从生存分布的随机变量 X 的support都是 (0,\\infin), 所以 Y=\\log(X)的support必然是 (-\\infin,\\infin). 利用 证明-14 中的步骤即可证明:\n\n\n\nX \\in (0,\\infin)\nY=\\log(X) \\in (-\\infin,\\infin)\n\n\n\n\nExponential\nLog-Weibull\n\n\nWeibull\nExtreme value\n\n\nLogNormal\nNormal\n\n\nGamma\nLog-Gamma\n\n\n\n对服从 Gamma 分布的随机变量 X 取倒数: Y=1/X. 可以得到服从 Inverted gamma distribution:\nf_Y(y)=\\frac {e^{-\\frac 1 {\\beta y}}(\\frac 1 {\\beta y})^\\alpha}{y\\Gamma(\\alpha)} \\; \\text{ where y&gt;0} \n对于support为(0,1)的随机变量 X, 其CDF为 F(x) \\in (0,1). 所以其反函数 F^{-1}(u) \\in (0,1), 也是一个合格的CDF. 以此为CDF的随机变量 Y 就是 X 的 complementary distribution. 比如 X \\sim \\text{Standard power}(\\beta). 则X 的CDF为 F(x)=x^\\beta. 所以 F^{-1}(u)=u^{\\frac 1 \\beta}. 如果 Y 的CDF是 F^{-1}(y), 则其 PDF 为 \\frac d {dy} F^{-1}(y)=\\frac 1 \\beta x^{\\frac 1 \\beta -1}. 于是 Standard power distribution的 complementary distribution 就是其自身.\n\\square\n\n\n\n证明-16\n另外, 图-1 中的一些变换可以启发随机数生成器的设计. 比如 Box-Muller 算法, 将 \\text{Uniform}(0,1) 先变换到指数分布, 再到 \\chi^2分布, 再到标准正态分布, 最后利用缩放变换到所需的正态分布. 证明-16\nBox-Muller 算法是由两个独立的标准均匀分布随机变量 U_1, U_2 \\sim \\text{Uniform}(0,1) 出发, 利用一系列变换得到两个独立的服从标准正态分布的随机变量 X, Y\\sim N(0,1). 注意, 因为标准正态分布的 Inverse CDF 不是 closed form, 所以此处不能使用 证明-10 中的方法来生成随机数. Box-Muller 方法是利用了极坐标变换.\n对独立的随机变量 X, Y 采样得到 (x,y), 可以将其视为在直角坐标系中的点. 变换到极坐标有:\n\nX^2+Y^2=R^2 \\\\\nX=R \\cos\\theta \\\\\nY=R \\sin\\theta\n\n由于 X,Y\\sim N(0,1), 所以 X^2, Y^2 服从自由度为 1 的 \\chi^2 分布, 进而 R^2 服从自由度为 2 的 chi^2 分布. 由 证明-12 可知这个分布就是 \\text{Exp}(\\alpha=2) 的指数分布. 由 证明-10 可知, 这个分布可以从随机变量 Z \\sim \\text{Uniform}(0,1) 经由变换 f(z)=-2\\ln(1-z) 来得到. 令 U_1=1-Z \\sim \\text{Uniform}(0,1), 于是 R^2=-2\\ln {U_1}. 另一方面, 注意到 \\theta \\sim \\text{Uniform}(0, 2\\pi), 于是:\n\nR=\\sqrt{-2\\ln{U_1}} \\\\\n\\theta=2\\pi U_2\n\n把极坐标变换回笛卡尔坐标, 得到:\n\nX=R\\cos\\theta=\\sqrt{-2\\ln{U_1}} \\cos(2\\pi U_2) \\\\\nY=R\\sin\\theta=\\sqrt{-2\\ln{U_1}} \\sin(2\\pi U_2)\n\n这意味着, 只要从标准均匀分布独立的采样出两个样本 (u_1, u_2), 代入上式, 就可以获得两个独立的服从标准正态分布的样本 (x,y). 所以整个过程可以视为标准均匀分布先变换到指数分布, 再到 \\chi^2分布, 再到标准正态分布, 最后利用缩放变换 \\mu+\\sigma X \\sim N(\\mu, \\sigma^2) 到所需的正态分布.\n\\square\n\n\n\n证明-17\n这只在特定的分布参数下才成立. 设随机变量 X \\sim \\text{Exp}(\\lambda), Y=[X]. 也就是说, 比如 \\Pr(0 \\le X &lt; 1)=\\Pr(Y=0).\n\n\\begin{align*}\n\\Pr(Y=n) &= \\int_n^{n+1} f_X(x; \\lambda) dx \\\\\n&= \\int_n^{n+1} \\lambda e^{-\\lambda x} dx \\\\\n&= e^{-n\\lambda}(1-e^{-\\lambda}) \\;\\; n=0,1,2,...\n\\end{align*}\n\n容易验证, Y \\sim \\text{Geometric}(p=1 - e^{-\\lambda}).\n\\square\n\n\n\n证明-18\n令随机变量 Y=\\frac 1 {1 + (n_1/n_2)X}, \\text{ where } X \\sim F(n_1, n_2), \\; n_1&gt;0, n_2&gt;0, X&gt;0, Y \\in (0,1). 于是Y 的 CDF:\n\n\\begin{align*}\nF_Y(y) &= \\Pr(Y \\le y) = \\Pr(\\frac 1 {1 + (n_1/n_2)X} \\le y) \\\\\n&= \\Pr(X \\ge \\frac {n_2}{n_1}(\\frac 1 y - 1)) \\\\\n&= 1 - F_X(\\frac {n_2}{n_1}(\\frac 1 y - 1))\n\\end{align*}\n\n将 F-distribution 的CDF: F_X(x)=I_{\\frac {n_1 x}{n_1 x + n_2}}(\\frac {n_1} 2, \\frac {n_2} 2), 代入得:\n\n\\begin{align*}\nF_Y(y) &= 1 - I_{1-y}(\\frac {n_1} 2, \\frac {n_2} 2) \\\\\n&= I_y(\\frac {n_2} 2, \\frac {n_1} 2)\n\\end{align*}\n\n这正是分布 Beta distribution 的CDF. 所以 Y \\sim \\text{Beta}(\\frac {n_2} 2, \\frac {n_1} 2).\n\\square\n\n\n\n证明-19\nPower series distribution是一个重要分布, 很多离散分布都是它的特例. 这个分布的 PDF 为:\nf(x; c)=\\frac {a_x c^x} {g(c)} = \\frac {a_x c^x} {\\sum_{i=0}^\\infin a_i c^i}, \\text{ where } c&gt;0, x=0,1,2,...\n对应于 \\text{Binomial}(x;n,p). 当 g(c) = (1+c)^n, \\text{ where } c=\\frac p {1-p} 时,\n\n\\begin{align*}\nf(x; c) &= \\frac {a_x (\\frac p {1-p})^x} {(1-p)^{-n}} \\\\\n&= a_x p^x (1-p)^{n-x}\n\\end{align*}\n\n其中 a_x 是 (1+c)^n 的第 x 项系数, 正好对应了二项式系数. 所以二项分布是Power series distribution的特例.\n\\square\n\n\n\n证明-20\nvon Mises 分布是圆上的连续分布，通常用于描述方位或角度数据。其PDF为:\nf(\\theta;\\mu,\\kappa)=\\frac {e^{\\kappa \\cos(\\theta-\\mu)}}{2\\pi I_0(\\kappa)}, \\text{ where } \\theta \\in (0, 2\\pi)\n由于 \\cos(0)=1, 可知分布的概率密度主要集中在 \\theta \\approx \\mu 附近, 而且 \\kappa 越大越集中. 所以当 \\kappa \\gg 0 时, 可以用Taylor展开来分析 \\theta \\approx \\mu 时的局部行为. 将 \\cos(\\theta-\\mu) \\approx 1-\\frac 1 2 (\\theta-\\mu)^2 代入PDF得到:\n\n\\begin{align*}\nf(\\theta;\\mu,\\kappa)&\\approx \\frac {e^{\\kappa(1 - \\frac 1 2 (\\theta-\\mu)^2)}}{2\\pi I_0(\\kappa)} \\\\\n&=\\frac {e^{-\\frac {\\kappa (\\theta - \\mu)^2} 2 }} {2\\pi e^{-\\kappa} I_0(\\kappa)} \\\\\n&=\\frac 1 {2\\pi e^{-1/\\sigma^2} I_0(1/\\sigma^2)} e^{-\\frac {(\\theta-\\mu)^2} {2\\sigma^2}} & \\text{ let } \\kappa=1/\\sigma^2 \\\\\n&\\approx \\frac 1 {\\sqrt{2\\pi}\\sigma} e^{-\\frac {(\\theta-\\mu)^2} {2\\sigma^2}} & \\text { normalize with } \\theta \\in \\R\n\\end{align*}\n\n这正是 Normal 分布.\n\\square\n\n\n\n证明-21\n\\chi 分布是定义在非负实数上的连续分布, 描述了多个独立的标准正态分布随机变量的平方和的平方根的分布, 亦即多元高斯随机变量与原点的欧式距离的分布. \\chi 分布有一个分布参数: 自由度. 当自由度为 1 时, X \\sim N(0,1), ; |X| (1)$. 由 \\Gamma(\\frac 1 2)=\\sqrt \\pi, 得到PDF:\nf_1(x) = \\sqrt {\\frac 2 \\pi} \\exp\\Big(-\\frac {x^2} 2\\Big), \\; x \\ge 0\n当自由度为 2 时, 对应 \\text{Rayleigh}(1) 分布. 可以描述二维平面内向量长度的分布, 比如机场跑道附近的风速, 随机复数的模. 都是假设随机向量在两个方向上独立的服从标准正态分布. 其PDF为:\nf_2(x) = x \\exp\\Big(-\\frac{x^2} 2\\Big), \\; x \\ge 0\n当自由度为 3 时, 对应 \\text{Maxwell-Boltzmann}(1) 分布. 可以描述三维空间种的向量长度的分布. 尤其是描述理想气体分子在平衡态下的速度或动能的分布. 由 \\Gamma(\\frac 3 2)=\\sqrt \\pi / 2, 得到PDF:\nf_3(x) = \\sqrt {\\frac 2 \\pi} x^2 \\exp\\Big(-\\frac {x^2} 2\\Big), \\; x \\ge 0\n正好是 Maxwell-Boltzmann分布在尺度参数 a=1 时的情形. 一般的, 这个分布参数为 a=\\sqrt {\\frac {kT} m}, 其中 k 是Boltzmann常数, T是气体温度, m 是分子质量. a^2与方差成正比. 所以相同温度下, 分子质量越小的理想气体, 其动能分布越分散.\nf_{MB}(v; a) = \\sqrt{\\frac 2 \\pi} \\frac {x^2} {a^3} \\exp\\Big(-\\frac {x^2} {2a^2}\\Big)=\\Big(\\frac m {2\\pi k T}\\Big)^{\\frac 3 2} 4\\pi v^2 \\exp\\Big(-\\frac {m v^2} {2kT}\\Big), \\; v \\ge 0\n\\square\n\n\n\n证明-22\nGamma分布的PDF:\nf(x; a, b)=\\frac 1 {a^b \\Gamma(b)} x^{b-1} e^{-x/a}, \\; x &gt; 0\nGeneralized gamma分布的PDF:\nf(x; a, b, c)=\\frac c {a^{bc} \\Gamma(b)} x^{bc-1} e^{-(x/a)^c}, \\; x &gt; 0\n可见Generalized主要体现在参数 c 上, 使得指数里面多了一层指数. 这个分布囊括了Gamma, \\chi, \\chi^2, exponential, Weibull分布. 论文中指出, 如果两个独立的随机变量 X,Y 分别服从参数为 (a_1, b_1, c), (a_2, b_2, c) 的generalized gamma分布, 则随机变量\nU=\\frac {T^c} {T^c + (a_1/a_2)^c} \\sim \\text{Beta}(b_1, b_2), \\; \\text{ where } T=\\frac X Y\n下面要证明这个结论。采用Jacobian变换并直接积分的方法。\n令 k=a_1/a_2, 随机变量 W=Y, 用 U,W 来表达 X,Y, 得到 X=(\\frac U {1-U})^{\\frac 1 c} k W. 于是联合概率密度:\n\n\\begin{align*}\nf_{U,W}(u, w) &= f_{X,Y}(x,y) \\cdot \\begin{Vmatrix}\\dfrac{\\partial (x, y)}{\\partial (u,w)}\\end{Vmatrix} \\\\\n&= f_{X,Y}\\Big((\\frac u {1-u})^{\\frac 1 c} k w, w\\Big) \\cdot \\begin{Vmatrix}\\dfrac{\\partial \\Big((\\frac u {1-u})^{\\frac 1 c} k w, w\\Big)}{\\partial (u,w)}\\end{Vmatrix} \\\\\n&= f_{X,Y}\\Big((\\frac u {1-u})^{\\frac 1 c} k w, w\\Big) \\cdot \\begin{Vmatrix} \\frac {kw}{cu(1-u)}(\\frac u {1-u})^{\\frac 1 c} & k (\\frac u {1-u})^{\\frac 1 c} \\\\ 0 & 1\\end{Vmatrix} \\\\\n&=f_{X,Y}\\Big((\\frac u {1-u})^{\\frac 1 c} k w, w\\Big) \\Big\\lvert \\frac {kw}{cu(1-u)}(\\frac u {1-u})^{\\frac 1 c} \\Big\\rvert \\\\\n&=f_{X,Y}(kws,w) \\Big\\lvert \\frac {kws}{cu(1-u)} \\Big\\rvert & \\text{ let } s=(\\frac u {1-u})^{\\frac 1 c}\n\\end{align*}\n\n于是 U 的概率密度可以由边缘概率求出:\n\n\\begin{align*}\nf_U(u) &= \\int_\\R f_{U,W}(u, w)dw \\\\\n&= \\int_0^\\infin f_{X,Y}(kws,w) \\Big\\lvert \\frac {kws}{cu(1-u)} \\Big\\rvert dw & \\because w=y &gt; 0 \\\\\n&=\\int_0^\\infin f_X(kws) f_Y(w) \\frac {kws}{cu(1-u)} dw & \\because X,Y \\text{independent}, u\\in(0,1), k,s,w,c&gt;0 \\\\\n&= \\frac {ks}{cu(1-u)} \\int_0^\\infin f(kws; a_1, b_1, c) f(w; a_2, b_2, c) wdw \\\\\n&= \\frac {ks}{cu(1-u)} \\int_0^\\infin \\frac c {a_1^{b_1 c} \\Gamma(b_1)} (kws)^{b_1 c -1}e^{- (kws / a_1)^c} \\frac c {a_2^{b_2 c} \\Gamma(b_2)} w^{b_2 c - 1} e^{-(w/a_2)^c} wdw \\\\\n&= \\frac {ks} {cu(1-u)} \\frac {c^2 (ks)^{b_1c-1}}{a_1^{b_1c}a_2^{b_2c}\\Gamma(b_1)\\Gamma(b_2)} \\int_0^\\infin w^{b_1c+b_2c-1}e^{-(kws/a_1)^c-(w/a_2)^c}dw \\\\\n&= \\frac {c s^{b_1c}} {u(1-u) {a_2^{b_1c + b_2c} \\Gamma(b_1)\\Gamma(b_2)}} \\cdot I & \\text{ where } I \\text{ is integral part}\\\\\n\\end{align*}\n\n看起来积分部分要能凑出个 \\Gamma(b_1+b_2). 令\nr=g(w)=(kws/a_1)^c+(w/a_2)^c=(w/a_2)^c(s^c+1)\n显然 g(w) 是递增函数, r\\in(0, \\infin). 于是\n\n\\begin{align*}\nw &= g^{-1}(r)=a_2 (\\frac r {s^c+1})^{\\frac 1 c} \\\\\ndw &= \\frac {a_2} c (s^c+1)^{-\\frac 1 c} r^{\\frac 1 c - 1}dr\n\\end{align*}\n\n代入积分部分 I 得到:\n\n\\begin{align*}\nI &= \\int_{g(0)}^{g(\\infin)} (a_2(\\frac r {s^c+1})^{\\frac 1 c})^{b_1c+b_2c-1} e^{-r} \\frac {a_2}c (s^c+1)^{-\\frac 1 c}r^{\\frac 1 c - 1}dr  \\\\\n&= a_2^{b_1c+b_2c} \\frac 1 c (s^c+1)^{-b_1-b_2} \\int_0^\\infin r^{b_1+b_2-1} e^{-r} dr \\\\\n&= a_2^{b_1c+b_2c} \\frac 1 c (s^c+1)^{-b_1-b_2} \\Gamma(b_1+b_2)\n\\end{align*}\n\n代入到 f_U(u) 得:\n\n\\begin{align*}\nf_U(u) &= \\frac {c s^{b_1c}} {u(1-u) {a_2^{b_1c + b_2c} \\Gamma(b_1)\\Gamma(b_2)}} a_2^{b_1c+b_2c} \\frac 1 c (s^c+1)^{-b_1-b_2} \\Gamma(b_1+b_2) \\\\\n&= \\frac {\\Gamma(b_1+b_2)} {\\Gamma(b_1)\\Gamma(b_2)} \\frac {s^{b_1c}(s^c+1)^{-b_1-b_2}}{u(1-u)} \\\\\n&= \\frac {\\Gamma(b_1+b_2)} {\\Gamma(b_1)\\Gamma(b_2)} u^{b_1-1} (1-u)^{b_2-1} & \\because s^c=\\frac u {1-u}\n\\end{align*}\n\n所以 U \\sim \\text{Beta}(b_1, b_2).\n特别的, 当 c=1 时, generalized gamma 分布退化为 gamma 分布. 再让 a_1=a_2=a, 可以得到:\n\\frac X {X+Y} \\sim \\text{Beta}(b_1, b_2)\n其中随机变量 X,Y 独立且 X \\sim \\text{Gamma}(a, b_1), Y \\sim \\text{Gamma}(a, b_2).\n\\square\n\n\n\n证明-23\n令随机变量 W=X&gt;0, U = \\frac Z {\\sqrt{X / n}} \\in \\R。 用 U, W 来表达 X,Z ，得到 Z=U \\sqrt{W/n} \\in \\R。 所以 U,W 的联合概率密度:\n\n\\begin{align*}\nf_{U,W}(u,w) &= f_{X,Z}(x,z) \\cdot \\begin{Vmatrix}\\dfrac{\\partial (x, z)}{\\partial (u,w)}\\end{Vmatrix} \\\\\n&= f_{X,Z}(w, u \\sqrt{w/n}) \\cdot \\begin{Vmatrix}\\dfrac{\\partial (w, u \\sqrt{w/n})}{\\partial (u,w)}\\end{Vmatrix} \\\\\n&= f_{X,Z}(w, u \\sqrt{w/n}) \\cdot \\begin{Vmatrix} 0 & 1 \\\\ \\sqrt{w/n} & \\frac u {2\\sqrt{nw}} \\end{Vmatrix} \\\\\n&= f_{X,Z}(w, u \\sqrt{w/n}) \\sqrt{w/n}\n\\end{align*}\n\n所以 U 的PDF可以由边缘概率得出：\n\n\\begin{align*}\nf_U(u) &= \\int_0^\\infin f_{X,Z}(w, u \\sqrt{w/n}) \\sqrt{w/n} \\, dw \\\\\n&= \\int_0^\\infin f_X(w) f_Z(u \\sqrt{w/n}) \\sqrt{w/n} \\, dw & \\because \\text{independent}  \\\\\n&= \\int_0^\\infin \\frac 1 {2^{n/2} \\Gamma(n/2)} w^{n/2-1}e^{-w/2} \\frac 1 {\\sqrt{2\\pi}} e^{-\\frac {u^2w} {2n}} \\sqrt{w/n} \\, dw \\\\\n&= \\frac {2^{-(n+1)/2}} {\\sqrt{n\\pi} \\Gamma(n/2)} \\int_0^\\infin w^{(n-1)/2} e^{-\\frac {(u^2+n)w}{2n}} \\, dw\n\\end{align*}\n\n令 t=g(w)=\\frac {(u^2+n)w}{2n}, 于是\n\n\\begin{align*}\nw &=\\frac {2nt} {u^2+n} \\\\\ndw &= \\frac {2n} {u^2+n} \\, dt\n\\end{align*}\n\n代入 f_U(u) 得到：\n\n\\begin{align*}\nf_U(u) &= \\frac {2^{-(n+1)/2}} {\\sqrt{n\\pi} \\Gamma(n/2)} \\int_{g(0)}^{g(\\infin)} \\Big(\\frac {2nt} {u^2+n}\\Big)^{(n-1)/2}e^{-t} \\frac {2n} {u^2+n} \\, dt \\\\\n&= \\frac {2^{-(n+1)/2}} {\\sqrt{n\\pi} \\Gamma(n/2)} \\Big(\\frac {2n} {u^2+n}\\Big)^{(n+1)/2} \\int_0^\\infin t^{(n+1)/2-1}e^{-t} \\, dt \\\\\n&= \\frac 1 {\\sqrt{n\\pi} \\Gamma(n/2)} \\Big(\\frac n {u^2+n}\\Big)^{(n+1)/2} \\Gamma((n+1)/2)\n\\end{align*}\n\n而这正是 t 分布的PDF。\n\\square\n\n\n\n证明-24\n以 \\delta 为非中心参数的非中心Beta分布可以定义为: \\frac X {X+Y}, 其中 X \\sim \\text{noncentral } \\chi^2(\\beta,\\delta), Y \\sim \\chi^2(\\gamma)\n令随机变量 U=\\frac X {X+Y} \\in (0,1), W=X&gt;0. 用 U,W 来表达 X,Y, 得到 Y=\\frac W U - W. 所以 U,W 的联合概率密度:\n\n\\begin{align*}\nf_{U,W}(u,w) &= f_{X,Y}(x,y) \\cdot \\begin{Vmatrix}\\dfrac{\\partial (x, y)}{\\partial (u,w)}\\end{Vmatrix} \\\\\n&= f_{X,Y}(w, \\frac w u - w)) \\cdot \\begin{Vmatrix}\\dfrac{\\partial (w, \\frac w u - w)}{\\partial (u,w)}\\end{Vmatrix} \\\\\n&= f_{X,Y}(w, \\frac w u - w) w u^{-2}\n\\end{align*}\n\n再利用 X, Y 的独立性, 所以 U 的PDF可以由边缘概率得出:\n\n\\begin{align*}\nf_U(u) &= \\int_0^\\infin f_X(w) f_Y(\\frac w u - w) w u^{-2} dw \\\\\n&= \\int_0^\\infin \\sum_{k=0}^\\infin \\frac {\\exp({-\\frac \\delta 2})(\\frac \\delta 2)^k}{k!} \\cdot \\frac {\\exp(- \\frac w 2) w^{k + \\frac \\beta 2 - 1}}{2^{k+ \\frac \\beta 2} \\Gamma(k + \\frac \\beta 2)} \\cdot \\frac 1 {2^{\\frac \\gamma 2} \\Gamma(\\frac \\gamma 2)} (\\frac w u -w)^{\\frac \\gamma 2-1} e^{-\\frac 1 2 (\\frac w u-w)}  w u^{-2} dw \\\\\n&= \\sum_{k=0}^\\infin \\Big(\\frac {e^{-\\frac \\delta 2}} {k!}\\Big) \\Big(\\frac \\delta 2 \\Big)^k\n\\frac {2^{-(k+ \\frac \\beta 2+\\frac \\gamma 2)}} {\\Gamma(k+ \\frac \\beta 2) \\Gamma(\\frac \\gamma 2)} \\cdot u^{-\\frac \\gamma 2-1} (1-u)^{\\frac \\gamma 2-1} \\cdot \\int_0^\\infin e^{-\\frac w {2u}} w^{k+ \\frac \\beta 2+ \\frac \\gamma 2-1} \\, dw \\\\\n&= \\sum_{k=0}^\\infin \\Big(\\frac {e^{-\\frac \\delta 2}} {k!}\\Big) \\Big(\\frac \\delta 2 \\Big)^k\n\\frac {2^{-(k+ \\frac \\beta 2+\\frac \\gamma 2)}} {\\Gamma(k+ \\frac \\beta 2) \\Gamma(\\frac \\gamma 2)} \\cdot u^{-\\frac \\gamma 2-1} (1-u)^{\\frac \\gamma 2-1} \\cdot 2^{k + \\frac \\beta 2+ \\frac \\gamma 2} u^{k + \\frac \\beta 2 + \\frac \\gamma 2} \\Gamma(k + \\frac \\beta 2 + \\frac \\gamma 2) \\\\\n&= \\sum_{k=0}^\\infin \\Big(\\frac {e^{-\\frac \\delta 2}} {k!}\\Big) \\Big(\\frac \\delta 2 \\Big)^k\n\\frac {\\Gamma(k + \\frac \\beta 2 + \\frac \\gamma 2)} {\\Gamma(k+ \\frac \\beta 2) \\Gamma(\\frac \\gamma 2)} u^{k+\\frac \\beta 2 - 1} (1-u)^{\\frac \\gamma 2 - 1}\n\\end{align*}\n\n而这个正是以 \\delta 为非中心参数, 以 \\frac \\beta 2, \\frac \\gamma 2 为形状参数的非中心Beta分布的PDF.\n\\square"
  },
  {
    "objectID": "posts/dirichlet_distribution/index.html",
    "href": "posts/dirichlet_distribution/index.html",
    "title": "Dirichlet distribution",
    "section": "",
    "text": "PDF:\n \\text{Dir}(\\vec x;\\vec α)=\\frac 1 {B(\\vec α)} \\prod_{i=1}^K x_i^{α_i-1}\n其中 K 是向量 \\vec x, \\vec α 的维度，\nB(\\vec \\alpha)=\\frac {\\prod_{i=1}^K \\Gamma(α_i)} {\\Gamma(\\sum_{i=1}^K α_i)}\n叫做 多元Beta函数，用于使整个分布的概率归一化。\nParameters:\n \\vec α=(α_1, ..., α_K)^T \\\\\n\\text{where } K \\ge 2 \\text{ and } α_1, ..., α_K &gt; 0\nSupport:\n \\vec x=(x_1, ..., x_K)^T \\\\\n\\text{where } x_1, ..., x_K \\in [0,1] \\text{ and } \\sum_{i=1}^K x_i=1"
  },
  {
    "objectID": "posts/dirichlet_distribution/index.html#定义",
    "href": "posts/dirichlet_distribution/index.html#定义",
    "title": "Dirichlet distribution",
    "section": "",
    "text": "PDF:\n \\text{Dir}(\\vec x;\\vec α)=\\frac 1 {B(\\vec α)} \\prod_{i=1}^K x_i^{α_i-1}\n其中 K 是向量 \\vec x, \\vec α 的维度，\nB(\\vec \\alpha)=\\frac {\\prod_{i=1}^K \\Gamma(α_i)} {\\Gamma(\\sum_{i=1}^K α_i)}\n叫做 多元Beta函数，用于使整个分布的概率归一化。\nParameters:\n \\vec α=(α_1, ..., α_K)^T \\\\\n\\text{where } K \\ge 2 \\text{ and } α_1, ..., α_K &gt; 0\nSupport:\n \\vec x=(x_1, ..., x_K)^T \\\\\n\\text{where } x_1, ..., x_K \\in [0,1] \\text{ and } \\sum_{i=1}^K x_i=1"
  },
  {
    "objectID": "posts/dirichlet_distribution/index.html#要点",
    "href": "posts/dirichlet_distribution/index.html#要点",
    "title": "Dirichlet distribution",
    "section": "要点",
    "text": "要点\n\n它是Beta分布的多元推广。当 K=2 时，退化为 Beta分布。\n它是一个连续分布，正参数向量 \\vec α 决定了分布的形态。\nDirichlet分布用于描述 \\text{Multinomial}(\\vec x; n,\\vec p) 中参数 \\vec p 的 不确定性，即 \\vec p 在不同取值下的可能性。\nDirichlet分布是Multinomial分布的共轭先验。"
  },
  {
    "objectID": "posts/dirichlet_distribution/index.html#解释",
    "href": "posts/dirichlet_distribution/index.html#解释",
    "title": "Dirichlet distribution",
    "section": "解释",
    "text": "解释\n先看Dirichlet分布在 K=2 时的情形：\n\\begin{align*}\n\\text{Dir}(x_1,x_2;α_1,α_2)&=\\frac 1 {B(α_1,α_2)} x_1^{α_1-1} x_2^{α_2-1} \\\\\n&=\\frac 1 {B(α_1,α_2)} x_1^{α_1-1} (1 - x_1)^{α_2-1} \\\\\n&=\\text{Beta}(x_1; α_1,α_2)\n\\end{align*}\n此时Dirichlet分布退化为Beta分布。如果有疑惑，建议先阅读 Beta distribution。\n这次对应的场景不再是扔硬币了，而是扔一个 K 面骰子（K=2 时对应两面骰子，即硬币）。我们想要了解抛这枚骰子后，各面出现的概率 \\vec p=(p_1,p_2,...,p_k)^T 来衡量这枚骰子的均匀程度。于是自然会要求 \\vec p 的每个分量的取值范围都是 [0,1]，且各分量之和等于 1：\\|\\vec p\\|_{1}=1 。在这种条件下，\\vec p 本身就代表了一种分布，描述了骰子各面出现的概率。从几何角度看，\\vec p 的所有可能取值构成了一个 K-1 维的 Simplex，嵌入在 K 维的单位正方体中。当 K=3 时，Simplex为嵌入在单位立方体中的正三角形，顶点为 (1,0,0),(0,1,0),(0,0,1)。\n\n\n\n红色三角形区域中的点代表了 \\vec p 的可能取值\n\n\n对于给定的 K 面骰子，以上帝的视角，\\vec p 是一个确定的常向量。而以我们的视角，就只能通过实际观测来推断它。利用贝叶斯推断，p 是一个 随机向量。在做实验之前，可以为其设定一个先验分布，用于表示对这枚骰子均匀程度的事先猜测。先验分布可以任意选取，在此我们刻意的选用Dirichlet分布: \\text{Dir}(\\vec p;\\vec α)。\n参数向量 \\vec α 决定了Dirichlet分布的形状。不同的参数搭配，可以产生相当丰富的分布形态，即选用Dirichlet分布可以表达出很多种先验假设（虽然不是全部）。\n\n如果 0 &lt; α_i &lt; 1，Dirichlet分布的概率密度集中在 Simplex 的顶点和边界，中心部分的概率密度很低，表示我们先验的认为骰子非常偏心，倾向于经常扔出某一面或某两面，其他面则很少出现；而且 α_i 越大，概率密度就越向对应分量集中。\n\n\n\n\n\n\n\n\n\n\n\\vec α=(0.2,0.2,0.2)\n\n\n\n\n\n\n\n\\vec α=(0.8,0.2,0.2)\n\n\n\n\n\n\n\n\\vec α=(0.8,0.8,0.2)\n\n\n\n\n\n\n如果 α_i &gt; 1，Dirichlet分布的概率密度集中在 Simplex 的中心附近，呈现钟形，表示我们先验的认为骰子相对均匀；而且 α_i 越大，概率密度就越向对应分量集中。\n\n\n\n\n\n\n\n\n\n\n\\vec α=(2,2,2)\n\n\n\n\n\n\n\n\\vec α=(8,2,2)\n\n\n\n\n\n\n\n\\vec α=(8,8,2)\n\n\n\n\n\n\n如果 \\vec α 的各个分量都相同时，Dirichlet分布关于 Simplex 中心对称。当 α_i = 1 时为均匀分布；而且 α_i 越大，概率密度就越向Simplex的中心集中。\n\n\n\n\n\n\n\n\n\n\n\\vec α=(1,1,1)\n\n\n\n\n\n\n\n\\vec α=(2,2,2)\n\n\n\n\n\n\n\n\\vec α=(8,8,8)\n\n\n\n\n\n通过实际扔几次骰子，统计各面出现的次数 D 后，就可以计算后验分布 \\Pr(\\vec p|D)，从而得到对 \\vec p 更有信心的估计：\n\\Pr(\\vec p|D)=\\frac {\\Pr(D|\\vec p)\\Pr(\\vec p)}{\\int_\\text{Simplex} \\Pr(D|\\vec p)\\Pr(\\vec p)d\\vec p} \\tag{1}\n其中\n\nD 表示实际扔 n 次骰子后，观察到各面朝上的次数分别为 \\vec a=(a_1, a_2,...a_K)，满足 \\sum_{i=1}^K a_i=n。\n\\Pr(\\vec p) 是先验分布：\\Pr(p)=\\text{Dir}(\\vec p;\\vec α) 。\n\\Pr(D|p) 是 Multinomial likelihood 1，是二项分布的扩展，表示已知骰子各面出现的几率为 \\vec p 时，产生出当前实验数据的概率。这个概率服从 Multinomial distribution: \\Pr(D|p)=\\frac {n!} {a_1!...a_K!} p_1^{a_1} p_2^{a_2}...p_K^{a_K}\n分母部分对 p 的所有可能取值积分，称作 全概率 或 配分函数（Partition function）。由于计算太复杂，一般情况下无法得到 closed form。但对于当前问题，为了方便表述，不妨设 K=3，代入 (1) 并化简：\n\n\n\\begin{align*}\n\\Pr(\\vec p|D)&=\\frac {\\frac {n!} {a_1!a_2!a_3!} p_1^{a_1} p_2^{a_2} p_3^{a_3} \\frac 1 {B(α_1, α_2, α_3)} p_1^{α_1-1} p_2^{α_2-1} p_3^{α_3-1}} {\\int_\\text{Simplex} \\frac {n!} {a_1!a_2!a_3!} p_1^{a_1} p_2^{a_2} p_3^{a_3} \\frac 1 {B(α_1,α_2,α_3)} p_1^{α_1-1} p_2^{α_2-1} p_3^{α_3-1} d \\vec p} \\\\\n&= \\frac {p_1^{a_1+α_1-1} p_2^{a_2+α_2-1} p_3^{a_3+α_3-1}} {\\int_{p_1=0}^1\\int_{p_2=0}^{1-p_1} {p_1^{a_1+α_1-1} p_2^{a_2+α_2-1} (1-p_1-p_2)^{a_3+α_3-1}} dp_1 dp_2}\\\\\n&=\\frac 1 {B(a_1+α_1,...,a_3+α_3)} p_1^{a_1+α_1-1} p_2^{a_2+α_2-1} p_3^{a_3+α_3-1} \\\\\n&=\\text{Dir}(\\vec p;\\vec α+\\vec a)\n\\end{align*}\n\n可见，后验分布仍然是 Dirichlet分布，只是参数分别加上了实验结果。所以，Dirichlet分布是 Multinomial分布的共轭先验。\n推荐一个在线可视化 Dirichlet 分布的网站。"
  },
  {
    "objectID": "posts/dirichlet_distribution/index.html#footnotes",
    "href": "posts/dirichlet_distribution/index.html#footnotes",
    "title": "Dirichlet distribution",
    "section": "脚注",
    "text": "脚注\n\n\nMultinomial distribution —— Wikipedia↩︎"
  },
  {
    "objectID": "posts/0.999...=1/index.html",
    "href": "posts/0.999...=1/index.html",
    "title": "0.999…=1",
    "section": "",
    "text": "参考\n\nhttps://terrytao.files.wordpress.com/2011/06/blog-book.pdf\nhttps://www.xjtu-blacksmith.cn/essay/real-numbers\nhttps://math.stackexchange.com/questions/11/is-it-true-that-0-999999999-ldots-1\nhttps://www.youtube.com/watch?v=jMTD1Y3LHcE\nhttps://www.youtube.com/watch?v=G_gUE74YVos&pp=ygULMC45OTk5Li4uPTE%3D\nhttps://web.archive.org/web/20141211053757/https://studentportalen.uu.se/uusp-filearea-tool/download.action?nodeId=754744&toolAttachmentId=145726\n\n为什么 0.999...=1？小时候我对此困惑不已。长大后发现不止是我，很多人都会觉得它冒犯了自己理性。对这个问题的讨论热度远远超其他数学命题。先说结论，要想证明 0.999...=1，就必须明确：\n\n什么是数\n数列极限的概念\n\n绕开这两件事的讨论都注定是错误的。下面先看两个常见的“证明”：\n\\begin{array}{cc}\n\n  \\begin{align*}\n  \\frac 1 3 &= 0.\\.3 &\\hbox{(by long division)} \\\\\n  3 \\times (\\frac 1 3) &= 3 \\times 0.\\.3 &\\hbox{(multiplying each digit by $3$)} \\\\\n  1 &= 0.\\.9\n  \\end{align*}\n\n& & &\n\n  \\begin{align*}\n  (9 + 1) \\times 0.\\.9 &= 9.\\.9 \\\\\n  9 \\times 0.\\.9 &= 9.\\.9 - 0.\\.9 \\\\\n  9 \\times 0.\\.9 &= 9 \\\\\n  0.\\.9 &= 1\n  \\end{align*}\n\n\\end{array}\n其中数字上方的点表示循环小数的循环节，比如 0.\\.12\\.3=0.123123...。这类证明常见于初高中的教科书中，前者使用算术方法，后者使用代数方法。看似合理，其实不然。\n第一个缺陷，还未确认符号 0.\\.3,\\; 0.\\.9 真的是数，就开始进行运算了。你可能会困惑，课本中都介绍了无限循环小数，那为何它们不是数呢？请考虑与之很相似的另一个“数” \\.9.，意在表示所有整数位都是9的“数”。于是可以构造下述推理：\n\\begin{align*}\n\n10 \\times \\.9. + 9 &= \\.9.  \\\\\n9 \\times \\.9. &= -9 \\\\\n\\.9. &= -1\n\n\\end{align*}\n这个推理跟前面的“证明”一样，每一步看着都对，但你会相信推出的结论吗？一个很大的正数会等于一个负数？显然不会吧。产生矛盾的原因就是误把 \\.9. 当作一个实际存在的数来运算。虽然在某些具有奇特结构的数系（Number system）中确实如此，但实数系（Real number system）不在此列。这个例子生动的表明，确实有必要确定符号 0.\\.3, 0.\\.9 的具体含义。\n第二个缺陷，将定义在有限位数上的算术操作直接应用于无限位数。为什么 10\\times 0.\\.9=9.\\.9 ？为什么 3\\times 0.\\.3=0.\\.9 ？对于有限位的十进制数，我们明确知道 \\times 10 就是把小数点向左移一位； \\times 3 就是从最低位开始，逐位的乘以3，并考虑向前的进位。但这些操作对于具有无限位的十进制数还成立吗？比如你都无法找到最低位，如何开始运算呢？就算能开始，因为有无穷位，你又如何结束整个运算呢？具体来说，对于乘法 0.\\.12\\.3 \\times 4 最低位是1，2，还是3？ 3\\times 4 的进位又如何处理？对于除法，如果承认 0.\\.9=1=9/ 9，则意味着下述奇怪的长除法是正确的：\n\\begin{array}{r}\n0.999... \\\\\n9 \\overline{\\smash{\\big)} 9.000...} \\\\\n\\underline{8\\phantom{.}1\\phantom{00...}} \\\\\n\\phantom{0.0}90\\phantom{0...} \\\\\n\\underline{\\phantom{0.0}81\\phantom{0...}} \\\\\n\\phantom{0.00}90\\phantom{...} \\\\\n\\underline{\\phantom{0.00}81\\phantom{...}} \\\\\n\\phantom{0.000}9... \\\\\n\\end{array}\n由此可见，不能将定义在有限位数下的运算规律想当然的推广到无限位数。对后者的操作，需要新的定义和新的运算规律，否则必然导致混乱和矛盾。这下可好，我们不止要证明 0.\\.9=1，就连课本上教的 0.\\.3=1/3 也得一起证明才行了。\n第三个缺陷，因果倒置。之前的“证明”之所以看起来每一步都对，是因为每一个等式的成立都基于先承认了 0.\\.9=1,\\; 0.\\.3=1/3 的正确性。也就是说，因为 0.\\.9=1,\\; 0.\\.3=1/3 成立，所以证明中的每个等式都成立。这种“因为我正确所以我正确” 的论述并非证明。数学证明的关键在于从一个步骤推理出另一个步骤的理由。这些理由要能形成连贯的逻辑链条，最终得出要证明的结论。只是罗列正确的等式是没用的，比如 “因为 90+45+45=180，60+60+60=180，所以三角形内角和为180°”，虽然每一句都正确，但这显然不是有效的证明。\n\n\n\n\n\n\n注记\n\n\n\n由此可见，混乱的根源在于没有搞清楚 0.\\.3,\\;0.\\.9 的定义。这些数学符号实际代表什么？符号本身没有特殊含义，除非你明确的赋予它。 0.\\.9 的定义肯定不是某种具有无限个小数位且每一位都是 9 的神奇小数。定义必须基于我们已经熟知的内容，比如有限位的数。\n\n\n0.\\.9 的真正定义是无穷数列 (0.9,\\;0.99,\\;0.999,\\;...) 的极限。这里涉及两个新名词，都需要给出明确的定义。先定义无穷数列：\n\n序列（Sequence）：由具有明确顺序且能逐一列举的对象组成的整体。比如 (昨天, 今天, 明天) 就是一个序列。整个序列记作 (a_n)，其中下标 n 代表对象的序号，从 1 开始编排。比如 a_1=昨天, \\; a_2=今天, \\; a_3=明天。排列的顺序很重要，(b_n)=(明天, 今天, 昨天) 就是与 (a_n) 不同的另一个序列。序列中每项的内容没有限制，比如 (1,X,\\{\\text{\"Hi\"},\\},(a_n),😀) 也是一个序列。但数轴上的所有点按所对应的数值从小到大排布所形成的列就不是序列，因为没法逐一列举。（ 0 的前一项和后一项是什么？）\n无穷序列（Infinite seqeunce）：项的个数不是有限的序列。注意定义中刻意避开了“无限”这个词。只要无法给出项的确切个数，就判定为无穷序列。\n数列（Number sequence）：每一项都是数的序列。于是项数不是有限个的数列就是无穷数列了。\n\n注意数列 (0.9,\\;0.99,\\;0.999,\\;...) 中的每一项都是有限位数的小数，所以都是定义明确的已知物，不涉及无限的概念。每一项都是对目标 0.\\.9 的近似或估计。具体来说，后一项都比前一项增加了一个小数位，使得越靠后的项，在形式上越接近那个目标——小数位全是9的“数”，显然 0.99 要比 0.9 更像它。\n极限可以通俗的解释为无穷数列最终趋近的数。比如数列(\\frac 1 2,\\; \\frac 3 4,\\; \\frac 7 8,\\; \\frac {15} {16},\\;...) 的极限是 1，这一点可以从 图 1 中获得直观感受：\n\n\n\n\n\n\n图 1: 几何级数\n\n\n\n并非每个无穷数列都有极限。比如偶数数列 (0,\\;2,\\;4,\\;6,\\;...) 是发散的，因为它趋向无穷大；比如数列 (0,\\;1,\\;0,\\;1,\\;...) 始终在 0 和 1 之间震荡，并不趋近于某个固定的数。这两种情况都称为不收敛。只有收敛的无穷数列才有极限。\n可见极限描述了数列中充分靠后的无穷项的整体性质，但在定义中只能使用有限的概念，所以极限的定义相当困难。数学家们进行了300多年的艰苦探索，最终由法国数学家柯西（Augustin-Louis Cauchy 1789-1857）完成：\n\n\n\n\n\n\n注记\n\n\n\n数列 (a_n) 的极限为实数 L，如果对于任意给定的正数 \\epsilon&gt;0，都存在一个正整数 N，使得对于任意的整数 n&gt;N，有\n|a_n-L|&lt;\\epsilon 。\n这意味着数列中足够靠后的项可以在数值上任意的接近 L，而不必考虑项的顺序。记作：\n\\lim_{n\\to +\\infin} a_n=L\n\n\n图 2 很好的描绘了极限定义的几何意义。\n\n\n\n\n\n\n图 2: 数列极限\n\n\n\n这个定义的特点在于，内容上只涉及有限性，每个部件都是含义明确的已知物，从而避免了与无限纠缠不清：\n\n\\epsilon 表达了逼近的精度。你可以任意选一个正数，比如 1,\\;0.2,\\;\\frac 3 {1000}，或者更小的数。\\epsilon 越小，就是要求数列中各项的数值波动范围越窄。但不管取多么小，每次都必须明确写出其数值，不能说让 \\epsilon 等于无限小。\n正整数 N 将数列分为两段，原数列的前 N 项构成一个有限数列，剩余部分构成一个新的无穷数列。\n“对任意 \\epsilon&gt;0，总存在正整数 N” 表达了一种确定性，即丢弃前 N 项后，剩余部分——那个新的无穷数列——中的每一项跟极限值 L 的差距都小于 \\epsilon，无一例外。注意，这个 N 是跟 \\epsilon 有关的。一般说来，\\epsilon 越小，不满足窄波动范围的项就越多，所需丢弃的项就越多，对应的 N 就越大。\n如果对于你能给出的每一个 \\epsilon 都能确定对应的 N ，则称数列 (a_n) 收敛到极限值 L 。如果不行，即你能给出一个具体的 \\epsilon，使得无论丢弃前面多少项，后面总会遇到某项的数值跟 L 的偏差大于 \\epsilon，那么 L 就不是 (a_n) 的极限。如果 任何一个确定的 L 都不是 (a_n) 的极限，就称 (a_n) 不收敛 或 没有极限。\n\n下面将这个定义应用到具体问题上，验证数列 (a_n)=(0.9,\\;0.99,\\;0.999,\\;...) 的极限是否为 1 ：\n比如若取 \\epsilon=\\frac 1 {10}=0.1，则可以令 N=1, 使得从 a_2 开始的每一项，都有 |a_n - 1|&lt;0.1 。 比如若取 \\epsilon=\\frac 1 {100}=0.01，则可以令 N=2，使得从 a_3 开始的每一项，都有 |a_n - 1|&lt;0.01 。 从这些特例可以发现规律，对于任意给定的 \\epsilon&gt;0 ，只要取 N 为 \\epsilon 所对应的十进制小数的第一个不为零的小数位所在的位数即可。用紧凑的公式可以表达为 N=-\\log_{10}(\\epsilon)。 这样就验证了 \\lim_{n\\to +\\infin} a_n=1。类似的，还可以验证数列 0.\\.3 所对应的数列 (b_n)=(0.3,\\;0.33,\\;0.333,\\;...) 的极限为 \\frac 1 3，以及其他无限循环小数比如 0.\\.12\\.3 所对应的数列 (c_n)=(0.1,\\;0.12,\\;0.123,\\;0.1231,\\;...) 的极限为 \\frac {123} {999}。\n至此，情况已经明朗起来：\n\n所有的分数（分子是整数，分母是不为零的整数）都有对应的十进制小数表达。一些分数对应到有限位的小数，另一些分数对应到无限循环小数。\n有限位的小数是由长除法得到的，刚好在某一位除尽了。无限循环小数也是由长除法得到，在长除的过程中出现之前遇到过的余数，从而发现商中的循环节，进而使用了比如 0.\\.12\\.3 这种符号来表示无限循环小数。要注意这仅仅是一种数学符号而已，它不能直接进行算术运算，否则就会带来混乱和矛盾。\n无限循环小数的精确定义是一个数列的极限。这个数列逐项增加相应的小数位数，而且数列的极限存在。\n\n但事情并未结束。因为就算经过这样严谨的定义，你心中一定还有困惑： 数列 (0.9,\\;0.99,\\;0.999,\\;...) 每一项都小于 1，为什么最后能等于 1 呢？这很可能是提问的根本动机。"
  },
  {
    "objectID": "drafts/trial/index.html",
    "href": "drafts/trial/index.html",
    "title": "什么是边际替代率?",
    "section": "",
    "text": "遍及替代率 (Marginal rate of substitution) 这个 fancy 的名词来自于经济学。用于描述在给定“总使用价值”，即给定效用（Utility）的前提下，人们在两种物品之间如何选择。"
  },
  {
    "objectID": "drafts/trial/index.html#概要",
    "href": "drafts/trial/index.html#概要",
    "title": "什么是边际替代率?",
    "section": "",
    "text": "遍及替代率 (Marginal rate of substitution) 这个 fancy 的名词来自于经济学。用于描述在给定“总使用价值”，即给定效用（Utility）的前提下，人们在两种物品之间如何选择。"
  },
  {
    "objectID": "drafts/trial/index.html#偏好preferences",
    "href": "drafts/trial/index.html#偏好preferences",
    "title": "什么是边际替代率?",
    "section": "偏好（Preferences）",
    "text": "偏好（Preferences）\n比如空姐询问，要喝茶水还是咖啡？人们会依据自身喜好给出回复。这就是偏好。经济学对于偏好有三个基本假设：\n\n完备性：人们对任意两种消费品 \\(A,B\\) ，总能确定是更喜欢A，还是更喜欢B，抑或同样喜欢，但不会出现无法确定的情况。\n传递性：如果偏好 \\(A \\le B\\) 且 \\(B \\le C\\)，则 \\(A \\le C\\) 。\n多多益善：拥有的消费品数量越多越好。相比于 1杯茶水+1杯咖啡，会更想拥有 2杯茶水+1杯咖啡。\n\n前两条假设就是数学中的全序关系（Total order) 。经济学模型中的很多重要结论，只用这两条假设就够了。第三条假设并非必要，但加入它会让推理得到大幅简化。这个假设等价于 “零成本处理掉多余消费品”，也算比较合理。"
  },
  {
    "objectID": "drafts/trial/index.html#效用函数-utility-function",
    "href": "drafts/trial/index.html#效用函数-utility-function",
    "title": "什么是边际替代率?",
    "section": "效用函数 (Utility function)",
    "text": "效用函数 (Utility function)\n上面所说的消费品并不限于具体的商品，可以是任何可以比较喜好的事物。为了精确简洁的描述人们对事物的偏好，可以将其数学化为 效用函数。这个函数可以把 不同种类不同数量的消费品 对应到 实数 上。于是人们对各组消费品的喜好顺序就可以用实数大小来排序。比如 \\(U(A)=1,U(B)=2,U(C)=10\\) 就表达了偏好：\\(A&lt;B&lt;C\\) 。\n\n\n\n\n\n\nWarning\n\n\n\n效用函数是 ordinal 而非 cardinal，所以返回值的大小只有相对意义，没有绝对意义。\\(U(C)=10U(A)\\) 只代表偏好 \\(C&gt;A\\)，并不意味着 \\(C\\) 比 \\(A\\) 好10倍。再以马拉松比赛为例，ordinal function返回的数值仅能用于排序，表达出前三名是谁，cardinal function返回的数值为跑步时长，不仅可以排序，还能表达如“第一名比第二名领先五分钟到达终点”这样的信息。"
  },
  {
    "objectID": "drafts/trial/index.html#边际效用-marginal-utility",
    "href": "drafts/trial/index.html#边际效用-marginal-utility",
    "title": "什么是边际替代率?",
    "section": "边际效用 (Marginal utility)",
    "text": "边际效用 (Marginal utility)\n边际效用描述了每增加一个单位的消费品所对应的效用增量。"
  },
  {
    "objectID": "drafts/count_bijection/index.html",
    "href": "drafts/count_bijection/index.html",
    "title": "计数的本质",
    "section": "",
    "text": "读完 George Gamow 的 《One Two Three… Infinity》，又看了两个相关的Youtube视频12，意犹未尽，把要点总结一下。大学里学过，计数的本质是建立两个集合元素之间的一一对应关系（ One-to-one correspondence），也就是找到双射（Bijection）。听着很有道理，但要真心理解，还是需要一些具体的例子，而且能有更多发现。"
  },
  {
    "objectID": "drafts/count_bijection/index.html#footnotes",
    "href": "drafts/count_bijection/index.html#footnotes",
    "title": "计数的本质",
    "section": "脚注",
    "text": "脚注\n\n\nThe Most Important Counting Concept You’ve (Probably) Never Heard Of↩︎\nCardinality of the Continuum↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "drafts/covariance/index.html",
    "href": "drafts/covariance/index.html",
    "title": "Covariance",
    "section": "",
    "text": "为分析学习时长 T 与考试成绩 S 的关系，观察了三位学生，得到了如下数据：\n\n\n\n\n\n\n\n\n\n学生\n时长\n成绩\n\n\n\n\nA\n1\n60\n\n\nB\n3\n80\n\n\nC\n8\n70\n\n\n\n\n\n\n直觉上，学习时间越长，成绩应该越好，需要设计一个指标来度量这种关系。首先要确定“越长”“越好”是相对于什么而言的，一种简单的选择就是相对于“平均时长” \\bar T 与“平均成绩” \\bar S。\n拿A, B房间来说，令\nd_{AB}=(S_A - S_B)(R_A - R_B)=2&gt;0\n也就是以这两个点为对角顶点的有向矩形的面积。如果结果为正，表示对于A，B房间而言，面积与租金正相关。而\nd_{AC}=(S_A - S_C)(R_A - R_C)=-4&lt;0\n表示对于A，C房间而言，面积与租金负相关。通过把每一对房间的指标加总：\nd=d_{AB}+d_{AC}+d_{BC}=2-4-4=-6&lt;0\n可以表示样本整体的相关性：\n\n结果的正、负号表示正、负相关；\n绝对大小表示相关性的强弱。\n\n但这样计算有个缺点，就是每增加一个样本点，就需要计算它跟之前每个样本点的相关性。"
  },
  {
    "objectID": "drafts/covariance/index.html#观测样本",
    "href": "drafts/covariance/index.html#观测样本",
    "title": "Covariance",
    "section": "",
    "text": "为分析学习时长 T 与考试成绩 S 的关系，观察了三位学生，得到了如下数据：\n\n\n\n\n\n\n\n\n\n学生\n时长\n成绩\n\n\n\n\nA\n1\n60\n\n\nB\n3\n80\n\n\nC\n8\n70\n\n\n\n\n\n\n直觉上，学习时间越长，成绩应该越好，需要设计一个指标来度量这种关系。首先要确定“越长”“越好”是相对于什么而言的，一种简单的选择就是相对于“平均时长” \\bar T 与“平均成绩” \\bar S。\n拿A, B房间来说，令\nd_{AB}=(S_A - S_B)(R_A - R_B)=2&gt;0\n也就是以这两个点为对角顶点的有向矩形的面积。如果结果为正，表示对于A，B房间而言，面积与租金正相关。而\nd_{AC}=(S_A - S_C)(R_A - R_C)=-4&lt;0\n表示对于A，C房间而言，面积与租金负相关。通过把每一对房间的指标加总：\nd=d_{AB}+d_{AC}+d_{BC}=2-4-4=-6&lt;0\n可以表示样本整体的相关性：\n\n结果的正、负号表示正、负相关；\n绝对大小表示相关性的强弱。\n\n但这样计算有个缺点，就是每增加一个样本点，就需要计算它跟之前每个样本点的相关性。"
  },
  {
    "objectID": "drafts/covariance/index.html#例子",
    "href": "drafts/covariance/index.html#例子",
    "title": "Covariance",
    "section": "例子",
    "text": "例子\n例-1 三个独立随机变量 A,B,C 和 两个常量 q,r: \n\\begin{align*}\nX &=qA+B \\\\\nY &=rA+C\n\\end{align*}\n 求 \\text{cov}(X,Y) 。\n解:\n由期望值的 Linearity，E[X]=qE[A]+E[B], \\; E[Y]=rE[A]+E[C] 。\n对于独立随机变量 A,B ，有 p_{A,B}(a,b)=p_A(a)p_B(b) ，于是\nE[AB]=\\int_A\\int_B p_{A,B}(a,b)abdadb=\\int p_A(a) a da \\cdot\\int p_B(b) b db=E[A]E[B]\n将以上两个结论代入协方差的计算中，得到： \n\\begin{align*}\n\\text{cov}(X,Y)&=E[XY]-E[X]E[Y] \\\\\n  &= E[(qA+B)(rA+C)] - (qE[A]+E[B])(rE[A]+E[C]) \\\\\n  &= E[qrA^2+qAC+rAB+BC] - (qE[A]+E[B])(rE[A]+E[C]) \\\\\n  &= qr E[A^2] + qE[A]E[C] + rE[A]E[B] + E[B]E[C] - (qrE[A]^2 + rE[A]E[B] + qE[A]E[C] + E[B]E[C]) \\\\\n  &= qr(E[A^2] - E[A]^2) \\\\\n  &= qr \\text{var}[A]\n\\end{align*}"
  },
  {
    "objectID": "drafts/covariance/index.html#定义",
    "href": "drafts/covariance/index.html#定义",
    "title": "Covariance",
    "section": "定义",
    "text": "定义\nPDF:\n \\text{Beta}(x;\\alpha, \\beta)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {B(\\alpha,\\beta)} \n其中\n B(\\alpha,\\beta)=\\int_0^1 u^{\\alpha-1}(1-u)^{\\beta-1} du=\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}  叫做 Beta函数, 用于使整个分布的概率归一化。\nParameters:\n \\alpha &gt;0, \\beta&gt;0 \nSupport:\n x \\in [0, 1]"
  },
  {
    "objectID": "drafts/covariance/index.html#要点",
    "href": "drafts/covariance/index.html#要点",
    "title": "Covariance",
    "section": "要点",
    "text": "要点\n\n这是一个连续分布。两个正参数 α,β 决定了分布的形态。\nBeta分布用于描述二项分布 \\text{Binomial}(x; n,p) 中参数 p 的 不确定性，即 p 在不同取值下的可能性。\nBeta分布是二项分布的共轭先验。"
  },
  {
    "objectID": "drafts/covariance/index.html#解释",
    "href": "drafts/covariance/index.html#解释",
    "title": "Covariance",
    "section": "解释",
    "text": "解释\n以扔硬币为例。若想要了解一个硬币是不是公平的，可以用正面朝上的概率 p 来衡量这枚硬币的均匀程度。以上帝的视角，p 是一个确定的常数。而以我们的视角，则只能通过实际观测来推断它。利用贝叶斯推断，p 是一个随机变量，取值范围为 [0,1] 。 p 配有一个先验分布，用于表示对其各种可能取值的事先猜测。先验分布可以任意选取，在此我们刻意的选用Beta分布。\n两个正参数 α 和 β 决定了Beta分布的形状。不同的参数搭配，可以产生相当丰富的分布形态，即选用Beta分布可以表达出很多种先验假设（虽然不是全部）。\n\n如果 α &gt; 1, β &gt; 1，Beta分布的形状将呈现一个钟形，表示我们先验的认为不均匀的程度更可能在峰值位置附近。\n\n\n\n\nα &gt; 1, β &gt; 1\n\n\n\n如果 α &lt; 1, β &lt; 1，Beta分布的形状在 0 和 1 附近呈现两个尖峰，表示我们先验的认为硬币非常偏心，倾向于 经常扔出正面 或 经常扔出反面。\n\n\n\n\nα &lt; 1, β &lt; 1\n\n\n\n如果 α = β，Beta分布关于 0.5 对称。当 α=β = 1 时为均匀分布。\n\n\n\n\nα = β\n\n\n通过实际扔几次硬币，获得正反面出现的次数 D 后，就可以计算后验分布 \\Pr(p|D)，从而得到对 p 更有信心的估计：\n\\Pr(p|D)=\\frac {\\Pr(D|p)\\Pr(p)}{\\int_0^1 \\Pr(D|p)\\Pr(p)dp} \\tag{1}\n其中\n\nD 表示实际扔 n 次硬币后，观察到有 a 次正面朝上，b 次反面朝上，a+b=n。\n\\Pr(p) 是先验分布：\\Pr(p)=\\text{Beta}(p;\\alpha,\\beta) 。\n\\Pr(D|p) 是 Binomial likelihood 1，表示如果硬币的不均匀性为 p，则产生出当前实验数据的可能性为 \\Pr(D|p)=C_{a+b}^a p^a (1-p)^b\n分母部分对 p 的所有可能取值积分，称作 全概率 或 配分函数（Partition function）。由于计算太复杂，一般情况下无法得到 closed form 2。但对于当前问题，代入 (1) 并化简：\n\n\n\\begin{align*}\n\\Pr(p|D)&=\\frac {C_{a+b}^a p^a (1-p)^b \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}} {B(\\alpha,\\beta)}} {\\int_0^1 C_{a+b}^a p^a (1-p)^b \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}} {B(\\alpha,\\beta)}dp} \\\\\n&=\\frac{p^{\\alpha-1+a}(1-p)^{\\beta-1+b}} {\\int_0^1 p^{\\alpha-1+a}(1-p)^{\\beta-1+b} dp} \\\\\n&=\\frac{p^{\\alpha-1+a}(1-p)^{\\beta-1+b}} {B(\\alpha+a, \\beta+b)} \\\\\n&=\\text{Beta}(p; \\alpha+a, \\beta+b)\n\\end{align*}\n\n神奇的事情发生了，后验分布仍然是 Beta分布，只是参数分别加上了实验结果。当先验分布与后验分布具有相同数学形式时，称为 共轭分布（Conjugate Distribution） ，称先验分布为似然函数 \\Pr(D|p) 的 共轭先验（Conjugate Prior）。这就是当初选择Beta分布作为先验的一个重要原因。如此选择就可以解析的计算出全概率，而且得到的后验分布形式不变，可以作为进一步实验的先验分布，从而大大简化计算。"
  },
  {
    "objectID": "drafts/covariance/index.html#footnotes",
    "href": "drafts/covariance/index.html#footnotes",
    "title": "Covariance",
    "section": "脚注",
    "text": "脚注\n\n\nBeta distribution —— Wikipedia↩︎\n《Beta分布，共轭先验与贝叶斯推断》 —— 于果↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "日积月累",
    "section": "",
    "text": "你应该知道的统计学概念\n\n\n\n\n\n\nMath\n\n\nProbability\n\n\n\n罗列一些重要的统计学概念，简述概念的含义与用法，作为日后参考使用。 \n\n\n\n\n\nMar 20, 2025\n\n\nShen Lei\n\n\n\n\n\n\n\n\n\n\n\n\n0.999…=1\n\n\n来自孩子的提问\n\n\n\nMathematics\n\n\n\n从多个角度解释等式成立的原因 \n\n\n\n\n\nJun 29, 2023\n\n\nShen Lei\n\n\n\n\n\n\n\n\n\n\n\n\n[译注] 单变量概率分布之间的关系\n\n\n原文链接: Univariate Distribution Relationships\n\n\n\nProbability theory\n\n\n\n各种概率分布并非孤立，而是紧密关联在一起的。本文基于原论文，总结了单变量概率分布的属性，同时对大部分的关联给出了详细的证明步骤。 \n\n\n\n\n\nJun 29, 2023\n\n\nShen Lei\n\n\n\n\n\n\n\n\n\n\n\n\nBeta distribution\n\n\n直观解释\n\n\n\nProbability theory\n\n\n\nBeta分布用于描述二项分布 \\(\\text{Binomial}(x; n,p)\\) 中参数 \\(p\\) 的 不确定性，即 \\(p\\) 在不同取值下的可能性。 \n\n\n\n\n\nMar 26, 2023\n\n\nShen Lei\n\n\n\n\n\n\n\n\n\n\n\n\nDirichlet distribution\n\n\n直观解释\n\n\n\nProbability theory\n\n\n\nDirichlet分布用于描述 \\(\\text{Multinomial}(\\vec x; n,\\vec p)\\) 中参数 \\(\\vec p\\) 的 不确定性，即 \\(\\vec p\\) 在不同取值下的可能性。 \n\n\n\n\n\nMar 26, 2023\n\n\nShen Lei\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/beta_distribution/index.html",
    "href": "posts/beta_distribution/index.html",
    "title": "Beta distribution",
    "section": "",
    "text": "PDF:\n \\text{Beta}(x;\\alpha, \\beta)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {B(\\alpha,\\beta)} \n其中\n B(\\alpha,\\beta)=\\int_0^1 u^{\\alpha-1}(1-u)^{\\beta-1} du=\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}  叫做 Beta函数, 用于使整个分布的概率归一化。\nParameters:\n \\alpha &gt;0, \\beta&gt;0 \nSupport:\n x \\in [0, 1]"
  },
  {
    "objectID": "posts/beta_distribution/index.html#定义",
    "href": "posts/beta_distribution/index.html#定义",
    "title": "Beta distribution",
    "section": "",
    "text": "PDF:\n \\text{Beta}(x;\\alpha, \\beta)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {B(\\alpha,\\beta)} \n其中\n B(\\alpha,\\beta)=\\int_0^1 u^{\\alpha-1}(1-u)^{\\beta-1} du=\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}  叫做 Beta函数, 用于使整个分布的概率归一化。\nParameters:\n \\alpha &gt;0, \\beta&gt;0 \nSupport:\n x \\in [0, 1]"
  },
  {
    "objectID": "posts/beta_distribution/index.html#要点",
    "href": "posts/beta_distribution/index.html#要点",
    "title": "Beta distribution",
    "section": "要点",
    "text": "要点\n\n这是一个连续分布。两个正参数 α,β 决定了分布的形态。\nBeta分布用于描述二项分布 \\text{Binomial}(x; n,p) 中参数 p 的 不确定性，即 p 在不同取值下的可能性。\nBeta分布是二项分布的共轭先验。"
  },
  {
    "objectID": "posts/beta_distribution/index.html#解释",
    "href": "posts/beta_distribution/index.html#解释",
    "title": "Beta distribution",
    "section": "解释",
    "text": "解释\n以扔硬币为例。若想要了解一个硬币是不是公平的，可以用正面朝上的概率 p 来衡量这枚硬币的均匀程度。以上帝的视角，p 是一个确定的常数。而以我们的视角，则只能通过实际观测来推断它。利用贝叶斯推断，p 是一个随机变量，取值范围为 [0,1] 。 p 配有一个先验分布，用于表示对其各种可能取值的事先猜测。先验分布可以任意选取，在此我们刻意的选用Beta分布。\n两个正参数 α 和 β 决定了Beta分布的形状。不同的参数搭配，可以产生相当丰富的分布形态，即选用Beta分布可以表达出很多种先验假设（虽然不是全部）。\n\n如果 α &gt; 1, β &gt; 1，Beta分布的形状将呈现一个钟形，表示我们先验的认为不均匀的程度更可能在峰值位置附近。\n\n\n\n\nα &gt; 1, β &gt; 1\n\n\n\n如果 α &lt; 1, β &lt; 1，Beta分布的形状在 0 和 1 附近呈现两个尖峰，表示我们先验的认为硬币非常偏心，倾向于 经常扔出正面 或 经常扔出反面。\n\n\n\n\nα &lt; 1, β &lt; 1\n\n\n\n如果 α = β，Beta分布关于 0.5 对称。当 α=β = 1 时为均匀分布。\n\n\n\n\nα = β\n\n\n通过实际扔几次硬币，获得正反面出现的次数 D 后，就可以计算后验分布 \\Pr(p|D)，从而得到对 p 更有信心的估计：\n\\Pr(p|D)=\\frac {\\Pr(D|p)\\Pr(p)}{\\int_0^1 \\Pr(D|p)\\Pr(p)dp} \\tag{1}\n其中\n\nD 表示实际扔 n 次硬币后，观察到有 a 次正面朝上，b 次反面朝上，a+b=n。\n\\Pr(p) 是先验分布：\\Pr(p)=\\text{Beta}(p;\\alpha,\\beta) 。\n\\Pr(D|p) 是 Binomial likelihood 1，表示如果硬币的不均匀性为 p，则产生出当前实验数据的可能性为 \\Pr(D|p)=C_{a+b}^a p^a (1-p)^b\n分母部分对 p 的所有可能取值积分，称作 全概率 或 配分函数（Partition function）。由于计算太复杂，一般情况下无法得到 closed form 2。但对于当前问题，代入 (1) 并化简：\n\n\n\\begin{align*}\n\\Pr(p|D)&=\\frac {C_{a+b}^a p^a (1-p)^b \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}} {B(\\alpha,\\beta)}} {\\int_0^1 C_{a+b}^a p^a (1-p)^b \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}} {B(\\alpha,\\beta)}dp} \\\\\n&=\\frac{p^{\\alpha-1+a}(1-p)^{\\beta-1+b}} {\\int_0^1 p^{\\alpha-1+a}(1-p)^{\\beta-1+b} dp} \\\\\n&=\\frac{p^{\\alpha-1+a}(1-p)^{\\beta-1+b}} {B(\\alpha+a, \\beta+b)} \\\\\n&=\\text{Beta}(p; \\alpha+a, \\beta+b)\n\\end{align*}\n\n神奇的事情发生了，后验分布仍然是 Beta分布，只是参数分别加上了实验结果。当先验分布与后验分布具有相同数学形式时，称为 共轭分布（Conjugate Distribution） ，称先验分布为似然函数 \\Pr(D|p) 的 共轭先验（Conjugate Prior）。这就是当初选择Beta分布作为先验的一个重要原因。如此选择就可以解析的计算出全概率，而且得到的后验分布形式不变，可以作为进一步实验的先验分布，从而大大简化计算。"
  },
  {
    "objectID": "posts/beta_distribution/index.html#footnotes",
    "href": "posts/beta_distribution/index.html#footnotes",
    "title": "Beta distribution",
    "section": "脚注",
    "text": "脚注\n\n\nBeta distribution —— Wikipedia↩︎\n《Beta分布，共轭先验与贝叶斯推断》 —— 于果↩︎"
  },
  {
    "objectID": "posts/Statistics Concepts You Should Know/index.html",
    "href": "posts/Statistics Concepts You Should Know/index.html",
    "title": "你应该知道的统计学概念",
    "section": "",
    "text": "数据是一个通用术语，指我们观察到的信息。数据有不同的类型：\n\n定量数据 (Quantitative data)：数值型的数据。有很多种。\n\n离散数据 (Discrete data)：比如用整数表示的数据。\n\n二元数据 (Binary data)：一类重要的离散数据，因为数值 0,1 可以表示 \\text{True}, \\text{False} 或者 \\text{On}, \\text{Off}。\n类别数据 (Categorical data)：可以看作是二元数据的进一步扩展，比如用 1,2,3 分别表示 A,B,C 三个不同的组或类别。\n\n连续数据 (Continuous data)：常用连续的实数表示的数据，比如身高、体重、等待某事件发生所需的时间 (Time-to-event data) 等。\n\n定性数据 (Qualitative data)：文字型的数据。"
  },
  {
    "objectID": "posts/Statistics Concepts You Should Know/index.html#footnotes",
    "href": "posts/Statistics Concepts You Should Know/index.html#footnotes",
    "title": "你应该知道的统计学概念",
    "section": "脚注",
    "text": "脚注\n\n\n100+ Statistics Concepts You Should Know↩︎"
  }
]